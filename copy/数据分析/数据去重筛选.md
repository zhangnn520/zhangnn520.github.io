---
sort: 2
---

# æ•°æ®å»é‡ç­›é€‰

> æŒç»­æ›´æ–°ä¸­


[ğŸ”¨ç®—æ³•å¼€å‘æ‰‹å†Œ](https://kg-nlp.github.io/Algorithm-Project-Manual/æ•°æ®åˆ†æ/æ•°æ®å»é‡ç­›é€‰.html)
[ğŸ”¨ä¸ªäººçŸ¥ä¹](https://www.zhihu.com/people/zhangyj-n)

## èƒŒæ™¯
* å…¬å¸ä¸šåŠ¡æ•°æ®åº“ä¸­æœ‰æˆç™¾ä¸Šåƒä¸‡çš„æ•°æ®,æ— è®ºæ˜¯ä¸šåŠ¡äººå‘˜è¿˜æ˜¯ç®—æ³•å¼€å‘éƒ½æ— æ³•ç›´æ¥ä½¿ç”¨è¿™äº›æ•°æ®,å¦‚æœèƒ½ä»ä¸­æå–å‡ºå‡ åƒæ¡æˆ–å‡ ä¸‡æ¡æœ‰ä»£è¡¨æ€§çš„æ•°æ®,å°±å¯ä»¥å¯åŠ¨ä¸šåŠ¡æ¢ç´¢æˆ–ç®—æ³•æ ‡æ³¨äº†
* å¯¹æ–‡æœ¬è¿›è¡Œèšç±»,å¯ä»¥å‹ç¼©æ–‡æœ¬é‡çº§,æå–å°‘é‡æ•°æ®å¯ä»¥è¿›è¡Œfew_shotæˆ–full_annotationè®­ç»ƒ
* LLMæ•°æ®åˆ†æ

## å·¥ä½œ

é€šè¿‡é˜…è¯»â¼¤è§„æ¨¡â½‚æœ¬èšç±»ç›¸å…³çš„èµ„æ–™ä»¥åŠå®è·µ,æ€»ç»“äº†ä»¥ä¸‹å‡ ç‚¹

* å¤§æ•°æ®é‡çš„å»é‡æˆ–å«èšç±»æ“ä½œé€šå¸¸åˆ†ä¸¤æ­¥:æ–‡æœ¬è¡¨å¾+èšç±»
* simhashè¿™ç§å±€éƒ¨æ•æ„Ÿå“ˆå¸Œç®—æ³•,é•¿æ–‡æœ¬ä¸é”™,çŸ­æ–‡æœ¬ä¸è¡Œ, å¾—åˆ°æ–‡æœ¬æŒ‡çº¹(1 1 0 1 0 1) å†é€šè¿‡æµ·æ˜è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦,åº”ç”¨æ­¥éª¤å¦‚ä¸‹
    *  åˆ†è¯,hashå€¼è®¡ç®—,ç‰¹å¾å‘é‡åŠ æƒ,åˆå¹¶,é™ç»´
    *  å°†simhashç­¾ååˆ†å—å­˜å‚¨,è®¡ç®—ç›¸ä¼¼åº¦;
    *  æŠŠæ‰€æœ‰è¯­æ–™è¿›è¡Œå­˜å‚¨åœ¨redisä¸­,æ–¹ä¾¿æ£€ç´¢
    *  simhashæœ€ç»ˆå¾—åˆ°çš„æ–‡æœ¬æŒ‡çº¹å¯ä»¥å½“åšæ˜¯ä¸€ä¸ªæ–‡æœ¬ç‰¹å¾å€¼,æœ€åèšç±»åº¦é‡çš„æ—¶å€™è®°å¾—ç”¨æµ·æ˜è·ç¦»
*  å‘é‡å¬å›çš„æœ¬è´¨æ˜¯å‘é‡èšç±»ï¼Œå†è¾…ä»¥é«˜æ•ˆçš„å‘é‡æŸ¥æ‰¾ï¼Œè¾¾åˆ°å‘é‡å¬å›çš„ç›®çš„
    * è¿™éƒ¨åˆ†å¯ä»¥çœ‹ä¸‹faissæˆ–milvusçš„æºç äº†è§£ä¸€ä¸‹
* æ–‡æœ¬å‘é‡åŒ–è¡¨å¾çš„æ–¹å¼æœ‰å¤šç§,tf-idf(è¯é¢‘-é€†æ–‡æ¡£ç»Ÿè®¡),Word2vec,é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å‘é‡ç­‰.
    * å‘é‡åŒ–çš„æ–¹æ³•,æš—å«äº†åˆ†å¸ƒå¼å‡è®¾,ä¸€å®šç¨‹åº¦è§£å†³åŒä¹‰è¯é—®é¢˜
* èšç±»çš„æ–¹æ³•æœ‰å¾ˆå¤š,ç»å¸¸ç”¨çš„æœ‰kmeans,å±‚çº§åŒ–èšç°‡,singlepass.
* éœ€è¦æ³¨æ„,æˆ‘ä»¬é€‰æ‹©çš„æ–‡æœ¬å‘é‡åŒ–è¡¨å¾çš„æ–¹å¼ä¸åŒ,é‚£ä½¿ç”¨çš„è·ç¦»åº¦é‡ä¹Ÿä¸åŒ:
    *   åƒsimhashé€‰æ‹©æµ·æ˜è·ç¦»,tf-idfçœ‹gensimæˆ–æ˜¯sklearnè‡ªå¸¦çš„ç›¸ä¼¼åº¦è®¡ç®—,å¯èƒ½å°±æ˜¯cos-sim;è¿˜æœ‰å…¶ä»–é¢„è®­ç»ƒç”Ÿæˆçš„å‘é‡å€¼,æˆ‘ä»¬å°±å‚è€ƒfaiss,milvusä¸­çš„è®¡ç®—ç›¸ä¼¼åº¦çš„æ–¹æ³•
    *   è¦æ³¨æ„å‘é‡ç©ºé—´ä¸­ç›¸ä¼¼åº¦çš„å®šä¹‰å’Œå¹³æ—¶åšçš„å‘é‡ç›¸ä¼¼åº¦çš„å®šä¹‰è¦æœ‰åŒºåˆ†:æœ‰å¯èƒ½ä¸€ä¸ªæ˜¯è®¡ç®—çš„æ¬§å¼è·ç¦»,å¦ä¸€ä¸ªè®¡ç®—çš„æ˜¯ç‚¹ç§¯
    *   ä¸ºäº†åŠ å¿«è·ç¦»é€Ÿåº¦,å¯ä»¥è€ƒè™‘å‘é‡å€¼çš„é‡åŒ–ä»¥åŠé™ç»´,è¿™å—å¯ä»¥å‚è€ƒå‘é‡æœç´¢å¼•æ“çš„å®˜æ–¹ä»‹ç»å’Œæºç 
*   ä»ä¼ä¸šä¸šåŠ¡æ•°æ®åº“ä¸­å¯¼å‡ºçš„æ•°æ®,å¤„ç†è¦èšç±»çš„æ–‡æœ¬,çœ‹ä¸€ä¸‹æœ‰æ²¡æœ‰å…¶ä»–çš„å­—æ®µ,å¤©ç„¶å¸¦æœ‰ç±»åˆ«æ ‡ç­¾,å¯ä»¥åˆ©ç”¨;ä½†è¿™ä¸ªæ ‡ç­¾çš„æ˜¯ä»å“ªä¸ªç»´åº¦è¿›è¡Œçš„åŒºåˆ†éœ€è¦åˆ†ææ˜¯å¦æœ‰ä»·å€¼
*   æˆ‘ä»¬æœ¬èº«ä½¿ç”¨çš„æ˜¯æ— ç›‘ç£èšç±»æ–¹æ³•,ç»“æœå¥½åä¸å®¹æ˜“è¯„ä»·,æˆ‘ä»¬é€šè¿‡è½®å»“ç³»æ•°æ³•,æˆ–æ˜¯æ‰‹è‚˜æ³•æ¥åˆ¤æ–­å„çº§åˆ†ç±»çš„å¥½åå³å¯
*   å¯ä»¥é‡‡ç”¨åˆ†çº§èšç±»çš„æ–¹æ³•,æé«˜æ•ˆç‡
    * æ¯”å¦‚ä¸€çº§å¯ä»¥åŸºäºæ–‡æœ¬å­—é¢æ£€ç´¢,é€šè¿‡å…³é”®è¯èšç±»(ç²—ç²’åº¦çš„èšç±»æ–¹å¼),äºŒçº§å¤šä¸Šé¢åˆ†å¥½çš„æ¡¶è¿›è¡Œç»†ç²’åº¦èšç±»,å¯ä»¥é€šè¿‡å‘é‡è¡¨å¾è®¡ç®—ç›¸ä¼¼åº¦èšç±»
*   é—®é¢˜:
    *    çŸ­æ–‡æœ¬èšç±»ç°‡çš„æ•°é‡å¾€å¾€ç‰¹åˆ«å¤§
    *    èšç±»æ•ˆæœå—åˆ°å‘é‡åŒ–å¥½åçš„å½±å“,è¿˜æœ‰èšç±»ç®—æ³•çš„å½±å“


## æ ¸å¿ƒä»£ç åŠŸèƒ½ä»‹ç»

æ•´ä¸ªå·¥ç¨‹å:large_scale_text_clustering,åŒ…å«ä»¥ä¸‹ç›®å½•
> conf æ˜¯é…ç½®æ–‡ä»¶
> core æ˜¯æ ¸å¿ƒé€»è¾‘å‡½æ•°
> dataset æ˜¯è¦å»é‡å‰åçš„åŸæ•°æ®
> db æ˜¯é¢†åŸŸè¯å…¸,åœç”¨è¯ç­‰

```
|-- conf
|   |-- __init__.py
|   |-- config.py
|   `-- logging_conf.py
|-- core
|   |-- __init__.py
|   |-- clustering_algorithm.py
|   |-- descend_dim.py
|   |-- model.py
|   |-- source_analysis.py
|   |-- statistic_analysis.py
|   |-- text_representation.py
|   `-- utils.py
|-- dataset
|-- db
|   |-- dict_new.txt
|   |-- hit_stopwords.txt
|   `-- zj_userdict.dict

```

### conf

```python
import sys
import os
current_dir = os.path.abspath(os.path.dirname(__file__))
root_dir = current_dir.replace('conf','')
sys.path.append(root_dir)

from conf.logging_conf import log_conf
dataset_dir = current_dir.replace('conf','dataset')
images_dir = current_dir.replace('conf','images')
model_dir = current_dir.replace('conf','model')
ptm_dir = current_dir.replace('conf','ptm')
log_dir = current_dir.replace('conf','log')
res_dir = current_dir.replace('conf','result')
db_dir = current_dir.replace('conf','db')
ptm_dir = root_dir.replace('CustomModule/large_scale_text_clustering','')

log_conf(os.path.join(log_dir,'statistic_analysis.log'))
```


```python
import logging
from logging.handlers import RotatingFileHandler
import os

def log_conf(log_path):

    LOG_FORMAT = "%(asctime)s - %(filename)s - %(lineno)d -  %(levelname)s - %(message)s"
    logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)

    log_file_handler = RotatingFileHandler(filename=log_path, maxBytes=1024 * 1024*10,
                                           backupCount=1)
    # è®¾ç½®æ—¥å¿—æ‰“å°æ ¼å¼
    formatter = logging.Formatter(LOG_FORMAT)
    log_file_handler.setFormatter(formatter)
    logging.getLogger('').addHandler(log_file_handler)
    logging.getLogger("transformers").setLevel(logging.INFO)
    logging.getLogger("jieba").setLevel(logging.WARNING)

```


### clustering_algorithm


* ä½¿ç”¨minibatch,ä½¿ç”¨sklearnè®¡ç®—çš„tfidfæ¨¡å‹
* ä½¿ç”¨hashç‰¹å¾è¿›è¡Œèšç±»
* ä½¿ç”¨single_passè¿›è¡Œèšç±»,è¾“å…¥å¯ä»¥æ˜¯tfidf,å¯ä»¥æ˜¯æ–‡æœ¬å‘é‡


```python
# -*- coding:utf-8 -*-
'''
# ç›®æ ‡ä»»åŠ¡ :
å‚è€ƒ # https://blog.csdn.net/qq_37967241/article/details/116721067
'''
from sklearn.cluster import MiniBatchKMeans,KMeans
from sklearn.cluster import AgglomerativeClustering as aggl
from sklearn import metrics
import gensim
from tqdm import tqdm
import numpy as np
import codecs
import pandas as pd
from conf.config import *
import logging
import json
logging.getLogger().setLevel(logging.WARNING)

'''ä½¿ç”¨minibatch'''
def mini_cluster(tfidf_model,num_cluster,source_file,target_file):
    '''
    :param tfidf_model:  ä½¿ç”¨sklearnè®¡ç®—çš„tfidfæ¨¡å‹
    :param num_cluster:  ç›®æ ‡æ•°ç›®
    :param source_file:  åŸæ–‡æœ¬æ–‡ä»¶
    :param target_file:  èšç±»åæ–‡ä»¶
    :return:
    '''
    minikm_tfidf_lsa = MiniBatchKMeans(n_clusters=num_cluster,init='k-means++',init_size=1024*3,batch_size=1024).fit(tfidf_model)
    label = minikm_tfidf_lsa.labels_
    centroid = minikm_tfidf_lsa.cluster_centers_
    minikm_tfidf_lsa.fit_predict(tfidf_model)  # æ¯ç¯‡çš„ç±»åˆ«
    np.savetxt(u'/Users/sunmengge/Desktop/scopus_content/aero_conf_cluster_center_600.txt', minikm_tfidf_lsa.cluster_centers_)  # è¾“å‡ºå„ä¸ªèšç±»ç°‡çš„è´¨å¿ƒ
    num_clusters = []
    cluster = minikm_tfidf_lsa.labels_tolist()
    result = codecs.open('','w')
    for label in cluster:
        result.write(str(label) + '\n')
    result.close()

    # æ¨¡å‹ç»“æœé¢„æµ‹è¯„ä»·
    score_res = metrics.silhouette_score(tfidf_model,label,metric='euclidean')
    logging.info('èšç±»ç»“æœ%s'%(str(score_res)))


'''ä½¿ç”¨hashç‰¹å¾è¿›è¡Œèšç±»'''
def hash_cluster(hash_file,num_cluster,source_file,target_file):
    '''
    :param hash_file: hashæŒ‡çº¹æ–‡ä»¶
    :param num_cluster: èšç±»æ•°ç›®
    :param source_file:  # åŸæ–‡æœ¬æ–‡ä»¶
    :param target_file:  # èšç±»åæ–‡ä»¶
    :return:
    '''
    hash_path = os.path.join(dataset_dir,hash_file)
    df_data = pd.read_csv(hash_path,sep='\t',nrows=1000)
    simhash_list = df_data['simhash'].values.tolist()
    simhash_list = [[float(i)] for i in simhash_list]
    res = KMeans(n_clusters=num_cluster,init='k-means++',n_init=10).fit(simhash_list,y=None)  # æ¯è¡Œæ–‡æœ¬èšç±»çš„æ ‡ç­¾ [56,1]
    res_list = res.labels_.tolist()
    # print(res.labels_.tolist())
    target_file = os.path.join(res_dir,target_file)
    total_list = []
    for ind,raw in tqdm(enumerate(df_data['text'].values.tolist())):
        total_list.append([raw,res_list[ind]])
    df = pd.DataFrame(total_list,columns=['text','label'])
    df.to_csv(target_file,sep='\t',index=False)
    # df.to_excel(target_file,index=False)
    logging.info(('hashèšç±»å®Œæˆ'))

'''ä½¿ç”¨single_passè¿›è¡Œèšç±»,è¾“å…¥å¯ä»¥æ˜¯tfidf,å¯ä»¥æ˜¯æ–‡æœ¬å‘é‡'''

class single_pass():
    def __init__(self):
        ''''''
    def get_tfidf_similarity(self, cluster_cores, vector):
        max_value = 0
        max_index = -1
        for k, core in cluster_cores.items():
            similarity = gensim.matutils.cossim(vector, core)
            if similarity > max_value:
                max_value = similarity
                max_index = k
        return max_index, max_value
    def get_doc2vec_similarity(self, cluster_cores, vector):
        max_value = 0
        max_index = -1
        for k, core in cluster_cores.items():
            similarity = metrics.pairwise.cosine_similarity(vector.reshape(1, -1), core.reshape(1, -1))
            similarity = similarity[0, 0]
            if similarity > max_value:
                max_value = similarity
                max_index = k
        return max_index, max_value
    def tfidf_single_pass(self,corpus_vec,corpus_id_list,vocab,theta):
        '''
        :param corpus_vec:  å‘é‡æ–‡ä»¶gensimç”Ÿæˆçš„å‘é‡
        :param corpus_id_list:  å¯¹åº”å†…å®¹çš„idåˆ—è¡¨
        :param vocab:  è¯å…¸ {int:str}
        :param theta:  é˜ˆå€¼
        :return:  ä½¿ç”¨tfidfè¿›è¡Œèšç±»
        '''
        clusters = {}
        cluster_cores = {}
        cluster_text = {}
        num_topic = 0
        cnt = 0
        for vector, text in tqdm(zip(corpus_vec, corpus_id_list), desc='èšç±»ä¸­...'):
            if num_topic == 0:
                clusters.setdefault(num_topic, []).append(vector)
                cluster_cores[num_topic] = vector
                cluster_text.setdefault(num_topic, []).append(text)
                num_topic += 1
            else:
                max_index, max_value = self.get_tfidf_similarity(cluster_cores, vector)
                if max_value > theta:
                    clusters[max_index].append(vector)
                    text_matrix = gensim.matutils.corpus2dense(clusters[max_index], num_terms=len(vocab),
                                                        num_docs=len(clusters[max_index])).T  # ç¨€ç–è½¬ç¨ å¯†
                    core = np.mean(text_matrix, axis=0)  # æ›´æ–°ç°‡ä¸­å¿ƒ
                    core = gensim.matutils.any2sparse(core)  # å°†ç¨ å¯†å‘é‡coreè½¬ä¸ºç¨€ç–å‘é‡
                    cluster_cores[max_index] = core
                    cluster_text[max_index].append(text)
                else:  # åˆ›å»ºä¸€ä¸ªæ–°ç°‡
                    clusters.setdefault(num_topic, []).append(vector)
                    cluster_cores[num_topic] = vector
                    cluster_text.setdefault(num_topic, []).append(text)
                    num_topic += 1
            cnt += 1
            if cnt % 10000 == 0:
                logging.info('num_tops {}...'.format(num_topic)+'\t'+'processing {}...'.format(cnt))
        return clusters, cluster_text

    def doc2vec_single_pass(self, corpus_vec, corpus_id_list, theta):
        '''
        :param corpus_vec:
        :param corpus_id_list:
        :param theta:
        :return:  ä½¿ç”¨æ–‡æœ¬å‘é‡è¿›è¡Œèšç±»
        '''
        clusters = {}
        cluster_cores = {}
        cluster_text = {}
        num_topic = 0
        cnt = 0
        for vector, text in tqdm(zip(corpus_vec, corpus_id_list), desc='èšç±»ä¸­...'):
            if num_topic == 0:
                clusters.setdefault(num_topic, []).append(vector)
                cluster_cores[num_topic] = vector
                cluster_text.setdefault(num_topic, []).append(text)
                num_topic += 1
            else:
                max_index, max_value = self.get_doc2vec_similarity(cluster_cores, vector)
                if max_value > theta:  #
                    clusters[max_index].append(vector)
                    core = np.mean(clusters[max_index], axis=0)  # æ›´æ–°ç°‡ä¸­å¿ƒ
                    cluster_cores[max_index] = core
                    cluster_text[max_index].append(text)
                else:  # åˆ›å»ºä¸€ä¸ªæ–°ç°‡  é˜ˆå€¼å¤ªé«˜å¾ˆå®¹æ˜“é€ æˆå¤ªå¤šçš„æ–°ç°‡,æ—¶é—´å¤ªæ…¢
                    clusters.setdefault(num_topic, []).append(vector)
                    cluster_cores[num_topic] = vector
                    cluster_text.setdefault(num_topic, []).append(text)
                    num_topic += 1
            cnt += 1
            if cnt % 10000 == 0:
                print('num_tops {}...'.format(num_topic), 'processing {}...'.format(cnt))
        return clusters, cluster_text

    def save_cluster(self, method, index2corpus, cluster_text, cluster_path):
        '''
        :param method:  ä½¿ç”¨tfidfè¿˜æ˜¯æ–‡æœ¬å‘é‡
        :param index2corpus: {int:str}
        :param cluster_text:  æ¯ä¸ªèšç±»æ ‡ç­¾ä¸‹å¯¹åº”çš„æ–‡æœ¬id {int:[id,id]}
        :param cluster_path:  ç»“æœä¿å­˜è·¯å¾„
        :return:  ä¿å­˜èšç±»æ–‡ä»¶
        '''
        clusterTopic_list = sorted(cluster_text.items(), key=lambda x: len(x[1]), reverse=True)
        with open(cluster_path, 'w+', encoding='utf-8') as save_obj:
            for k in clusterTopic_list:
                data = dict()
                data["cluster_id"] = k[0]
                data["cluster_nums"] = len(k[1])
                data["cluster_docs"] = [{"doc_id": index, "doc_content": index2corpus.get(value)} for index, value in
                                        enumerate(k[1], start=1)]
                json_obj = json.dumps(data, ensure_ascii=False)
                save_obj.write(json_obj+'\n')
        logging.info('single_passèšç±»å®Œæˆ')

'''ä½¿ç”¨SSEæ›²çº¿åˆ¤æ–­æœ€ä½³ç±»åˆ«'''



if __name__ == '__main__':

    hash_cluster(hash_file='æ•°æ®-hash.tsv',num_cluster=10,source_file='',target_file='æ•°æ®-hashèšç±»ç»“æœ.xlsx')

    pass
```


### descend_dim

* ä½¿ç”¨SVDé™ç»´,éœ€è¦æŸ¥çœ‹å¯è§£é‡Šæ¯”ä¾‹,è¾“å…¥sklearnçš„tfidf
* ä½¿ç”¨lsiæ½œåœ¨è¯­ä¹‰åˆ†æ,è¾“å…¥gensimçš„tfidf
* ä½¿ç”¨ldaé™ç»´,è¾“å…¥gensimçš„tfidf


```python
# -*- coding:utf-8 -*-
import sys
import os
current_dir = os.path.abspath(os.path.dirname(__file__))
root_dir = current_dir.replace('core','')
sys.path.append(root_dir)

from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
import gensim
from collections import Counter
from conf.config import *
import logging


'''ä½¿ç”¨SVDé™ç»´,éœ€è¦æŸ¥çœ‹å¯è§£é‡Šæ¯”ä¾‹,è¾“å…¥sklearnçš„tfidf'''
def svd_descend(tfidf):
    n_components_list = [64,128,256,512,1024,2048]
    explained_variance_final = 0
    for n_components in n_components_list:
        svd = TruncatedSVD(n_components)
        normalizer = Normalizer(copy=False)
        lsa = make_pipeline(svd,normalizer)
        tfidf_lsa = lsa.fit_transform(tfidf)
        explained_variance = svd.explained_variance_ratio_.sum()
        if explained_variance > 0.8:
            n_components_final = n_components
            break
        if explained_variance > explained_variance_final:
            n_components_final = n_components
            explained_variance_final = explained_variance
    svd = TruncatedSVD(n_components_final)
    normalizer = Normalizer(copy=False)
    lsa = make_pipeline(svd, normalizer)
    tfidf_lsa = lsa.fit_transform(tfidf)
    explained_variance = svd.explained_variance_ratio_.sum()
    logging.info('è§£é‡Šæ–¹å·®æ¯”ä¾‹%f'%explained_variance)
    logging.info('svd-lsaé™ç»´åç»´åº¦%s'%(str(tfidf_lsa.shape)))
    return tfidf_lsa

'''ä½¿ç”¨lsiæ½œåœ¨è¯­ä¹‰åˆ†æ,è¾“å…¥gensimçš„tfidf'''
def lsi_descend(tfidf,dictionary,num_topics):
    lsi_model = gensim.models.LsiModel(tfidf,id2word=dictionary,num_topics=num_topics)
    corpus_lsi = lsi_model[tfidf]
    return corpus_lsi

'''ä½¿ç”¨ldaé™ç»´,è¾“å…¥gensimçš„tfidf'''
def lda_descend(tfidf,dictionary,num_topics):
    '''https://zhuanlan.zhihu.com/p/21364664'''
    lda_model = gensim.models.LdaModel(tfidf,id2word=dictionary,num_topics=num_topics)
    corpus_lda = lda_model[tfidf]
    return corpus_lda


'''æµ‹è¯•è„šæœ¬'''
def test():
    from core.statistic_analysis import sklearn_tfidf
    sklearn_tf_idf = sklearn_tfidf('æ•°æ®åˆ†è¯åˆ—è¡¨.txt')
    tfidf_model = sklearn_tf_idf.get_tfidf()
    svd_descend(tfidf_model)
if __name__ == '__main__':

    test()
    pass
```


### model

* TrieNodeèŠ‚ç‚¹æ ‘,ç®—å·¦å³ç†µå’Œäº’ä¿¡æ¯

```python
# -*- coding:utf-8 -*-
'''
---------------------------------
# ç›®æ ‡ä»»åŠ¡ :  é€šè¿‡èŠ‚ç‚¹æ ‘è®¡ç®—å·¦å³ç†µå’Œäº’ä¿¡æ¯
---------------------------------
'''
from conf.config import *
import math
import pandas as pd


class Node(object):
    """
    å»ºç«‹å­—å…¸æ ‘çš„èŠ‚ç‚¹
    """

    def __init__(self, char):
        self.char = char
        # è®°å½•æ˜¯å¦å®Œæˆ
        self.word_finish = False
        # ç”¨æ¥è®¡æ•°
        self.count = 0
        # ç”¨æ¥å­˜æ”¾èŠ‚ç‚¹
        self.child = []
        # æ–¹ä¾¿è®¡ç®— å·¦å³ç†µ
        # åˆ¤æ–­æ˜¯å¦æ˜¯åç¼€ï¼ˆæ ‡è¯†åç¼€ç”¨çš„ï¼Œä¹Ÿå°±æ˜¯è®°å½• b->c->a å˜æ¢åçš„æ ‡è®°ï¼‰
        self.isback = False


class TrieNode(object):
    """
    å»ºç«‹å‰ç¼€æ ‘ï¼Œå¹¶ä¸”åŒ…å«ç»Ÿè®¡è¯é¢‘ï¼Œè®¡ç®—å·¦å³ç†µï¼Œè®¡ç®—äº’ä¿¡æ¯çš„æ–¹æ³•
    """

    def __init__(self, node, data=None, PMI_limit=20):
        """
        åˆå§‹å‡½æ•°ï¼Œdataä¸ºå¤–éƒ¨è¯é¢‘æ•°æ®é›†
        :param node:
        :param data:
        """
        self.root = Node(node)
        self.PMI_limit = PMI_limit
        if not data:
            return
        node = self.root
        for key, values in data.items():
            new_node = Node(key)
            new_node.count = int(values)
            new_node.word_finish = True
            node.child.append(new_node)

    def add(self, word):
        """
        æ·»åŠ èŠ‚ç‚¹ï¼Œå¯¹äºå·¦ç†µè®¡ç®—æ—¶ï¼Œè¿™é‡Œé‡‡ç”¨äº†ä¸€ä¸ªtrickï¼Œç”¨a->b<-c æ¥è¡¨ç¤º cba
        å…·ä½“å®ç°æ˜¯åˆ©ç”¨ self.isback æ¥è¿›è¡Œåˆ¤æ–­
        :param word: ('æœºæ„',) æˆ– ('ç»“æ„', 'å‰¯æ¡†')
        :return:  ç›¸å½“äºå¯¹ [a, b, c] a->b->c, [b, c, a] b->c->a
        """
        node = self.root
        # æ­£å¸¸åŠ è½½
        for count, char in enumerate(word):
            found_in_child = False
            # åœ¨èŠ‚ç‚¹ä¸­æ‰¾å­—ç¬¦
            for child in node.child:
                if char == child.char:
                    node = child
                    found_in_child = True
                    break

            # é¡ºåºåœ¨èŠ‚ç‚¹åé¢æ·»åŠ èŠ‚ç‚¹ã€‚ a->b->c
            if not found_in_child:
                new_node = Node(char)
                node.child.append(new_node)
                node = new_node

            # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¿™ä¸ªè¯æ¯å‡ºç°ä¸€æ¬¡å°±+1
            if count == len(word) - 1:
                node.count += 1
                node.word_finish = True

        # å»ºç«‹åç¼€è¡¨ç¤º
        length = len(word)
        node = self.root
        if length == 3:
            word = list(word)
            word[0], word[1], word[2] = word[1], word[2], word[0]

            for count, char in enumerate(word):
                found_in_child = False
                # åœ¨èŠ‚ç‚¹ä¸­æ‰¾å­—ç¬¦ï¼ˆä¸æ˜¯æœ€åçš„åç¼€è¯ï¼‰
                if count != length - 1:
                    for child in node.child:
                        if char == child.char:
                            node = child
                            found_in_child = True
                            break
                else:
                    # ç”±äºåˆå§‹åŒ–çš„ isback éƒ½æ˜¯ Falseï¼Œ æ‰€ä»¥åœ¨è¿½åŠ  word[2] åç¼€è‚¯å®šæ‰¾ä¸åˆ°
                    for child in node.child:
                        if char == child.char and child.isback:
                            node = child
                            found_in_child = True
                            break

                # é¡ºåºåœ¨èŠ‚ç‚¹åé¢æ·»åŠ èŠ‚ç‚¹ã€‚ b->c->a
                if not found_in_child:
                    new_node = Node(char)
                    node.child.append(new_node)
                    node = new_node

                # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¿™ä¸ªè¯æ¯å‡ºç°ä¸€æ¬¡å°±+1
                if count == len(word) - 1:
                    node.count += 1
                    node.isback = True
                    node.word_finish = True

    def search_one(self):
        """
        è®¡ç®—äº’ä¿¡æ¯: å¯»æ‰¾ä¸€é˜¶å…±ç°ï¼Œå¹¶è¿”å›è¯æ¦‚ç‡
        :return:
        """
        result = {}
        node = self.root
        if not node.child:
            return False, 0

        # è®¡ç®— 1 gram æ€»çš„å‡ºç°æ¬¡æ•°
        total = 0
        for child in node.child:
            if child.word_finish is True:
                total += child.count
        # è®¡ç®— å½“å‰è¯ å æ•´ä½“çš„æ¯”ä¾‹
        for child in node.child:
            if child.word_finish is True:
                result[child.char] = child.count / total
        # print(result)  # æ¯ä¸ªè¯å‡ºç°çš„æ¦‚ç‡
        # print(total)  # æ•°é‡éå¸¸å¤§ 160001512
        return result, total

    def search_bi(self):
        """
        è®¡ç®—äº’ä¿¡æ¯: å¯»æ‰¾äºŒé˜¶å…±ç°ï¼Œå¹¶è¿”å› log2( P(X,Y) / (P(X) * P(Y)) å’Œè¯æ¦‚ç‡
        :return:
        """
        result = {}
        node = self.root
        if not node.child:
            return False, 0

        total = 0
        # 1 gram å„è¯çš„å æ¯”ï¼Œå’Œ 1 gram çš„æ€»æ¬¡æ•°
        one_dict, total_one = self.search_one()

        for child in node.child:
            for ch in child.child:
                if ch.word_finish is True:
                    total += ch.count
                    # print(ch.char, ch.count)
        # print(total)  # 183
        for child in node.child:
            for ch in child.child:
                if ch.word_finish is True:
                    # print(child.char,child.count,ch.char,ch.count,total)
                    # äº’ä¿¡æ¯å€¼è¶Šå¤§ï¼Œè¯´æ˜ a,b ä¸¤ä¸ªè¯ç›¸å…³æ€§è¶Šå¤§  PMIæ˜¯ç‚¹äº’ä¿¡æ¯,åªè®¡ç®—logå€¼
                    PMI = math.log(max(ch.count, 1), 2) - math.log(total, 2) - \
                          math.log(one_dict[child.char], 2) - math.log(one_dict[ch.char], 2)
                    # è¿™é‡Œåšäº†PMIé˜ˆå€¼çº¦æŸ
                    # if PMI > self.PMI_limit:
                    # ä¾‹å¦‚: dict{ "a_b": (PMI, å‡ºç°æ¦‚ç‡), .. }
                    result[child.char + '_' + ch.char] = (PMI, ch.count / total)
        return result

    def search_left(self):
        """
        å¯»æ‰¾å·¦é¢‘æ¬¡
        ç»Ÿè®¡å·¦ç†µï¼Œ å¹¶è¿”å›å·¦ç†µ (bc - a è¿™ä¸ªç®—çš„æ˜¯ abc|bc æ‰€ä»¥æ˜¯å·¦ç†µ)
        :return:
        """
        result = {}
        node = self.root
        if not node.child:
            return False, 0

        for child in node.child:
            for cha in child.child:
                total = 0
                p = 0.0
                for ch in cha.child:
                    if ch.word_finish is True and ch.isback:
                        total += ch.count
                for ch in cha.child:
                    if ch.word_finish is True and ch.isback:
                        p += (ch.count / total) * math.log(ch.count / total, 2)
                # è®¡ç®—çš„æ˜¯ä¿¡æ¯ç†µ
                result[child.char + cha.char] = -p
        return result

    def search_right(self):
        """
        å¯»æ‰¾å³é¢‘æ¬¡
        ç»Ÿè®¡å³ç†µï¼Œå¹¶è¿”å›å³ç†µ (ab - c è¿™ä¸ªç®—çš„æ˜¯ abc|ab æ‰€ä»¥æ˜¯å³ç†µ)
        :return:
        """
        result = {}
        node = self.root
        if not node.child:
            return False, 0

        for child in node.child:
            for cha in child.child:
                total = 0
                p = 0.0
                for ch in cha.child:
                    if ch.word_finish is True and not ch.isback:
                        total += ch.count
                for ch in cha.child:
                    if ch.word_finish is True and not ch.isback:
                        p += (ch.count / total) * math.log(ch.count / total, 2)
                # è®¡ç®—çš„æ˜¯ä¿¡æ¯ç†µ
                result[child.char + cha.char] = -p
        return result

    def find_pmi(self):
        '''åªè®¡ç®—ç‚¹äº’ä¿¡æ¯,åšä¸ºèšç±»åˆ¤æ–­ä½¿ç”¨'''
        bi = self.search_bi()
        result = {}
        for key, values in bi.items():
            result[key] = values[0]  # åªä¿ç•™pmi
        result = dict(sorted(result.items(), key=lambda x: x[1], reverse=True))

        return result

    def find_word(self, N):
        # é€šè¿‡æœç´¢å¾—åˆ°äº’ä¿¡æ¯
        # ä¾‹å¦‚: dict{ "a_b": (PMI, å‡ºç°æ¦‚ç‡), .. }
        bi = self.search_bi()
        # é€šè¿‡æœç´¢å¾—åˆ°å·¦å³ç†µ
        left = self.search_left()
        right = self.search_right()
        result = {}
        for key, values in bi.items():
            d = "".join(key.split('_'))
            # è®¡ç®—å…¬å¼ score = PMI + min(å·¦ç†µï¼Œ å³ç†µ) => ç†µè¶Šå°ï¼Œè¯´æ˜è¶Šæœ‰åºï¼Œè¿™è¯å†ä¸€èµ·å¯èƒ½æ€§æ›´å¤§ï¼  PMIè¶Šå¤§,è¶Šå‡å›º,ç†µè¶Šå°,è¶Šæœ‰åº
            if len(key) != len(d) + 1:
                print(key, values, d)
                continue
            result[key] = (values[0] + min(left[d], right[d])) * values[1]
        # æŒ‰ç…§ å¤§åˆ°å°å€’åºæ’åˆ—ï¼Œvalue å€¼è¶Šå¤§ï¼Œè¯´æ˜æ˜¯ç»„åˆè¯çš„æ¦‚ç‡è¶Šå¤§
        # resultå˜æˆ => [('ä¸–ç•Œå«ç”Ÿ_å¤§ä¼š', 0.4380419441616299), ('è”¡_è‹±æ–‡', 0.28882968751888893) ..]
        result = sorted(result.items(), key=lambda x: x[1], reverse=True)  # é™åº
        # print("result: ", result)
        # b = list(map(lambda x:[x[0],x[0].replace('_',''),x[1]],result))
        # df = pd.DataFrame(b, columns=['ngramåç§°', 'åˆå¹¶åç§°', 'åˆ†å€¼'])
        # df.to_excel(newWord_path,index=0)
        dict_list = [result[0][0]]
        # print("dict_list: ", dict_list)
        add_word = {}
        new_word = "".join(dict_list[0].split('_'))
        # è·å¾—æ¦‚ç‡
        add_word[new_word] = result[0][1]

        # å–å‰5ä¸ª
        # [('è”¡_è‹±æ–‡', 0.28882968751888893), ('æ°‘è¿›å…š_å½“å±€', 0.2247420989996931), ('é™ˆæ—¶_ä¸­', 0.15996145099751344), ('ä¹äºŒ_å…±è¯†', 0.14723726297223602)]
        for d in result[1: N]:
            flag = True
            for tmp in dict_list:
                pre = tmp.split('_')[0]
                # æ–°å‡ºç°å•è¯åç¼€ï¼Œåœ¨è€è¯çš„å‰ç¼€ä¸­ or å¦‚æœå‘ç°æ–°è¯ï¼Œå‡ºç°åœ¨åˆ—è¡¨ä¸­; åˆ™è·³å‡ºå¾ªç¯
                # å‰é¢çš„é€»è¾‘æ˜¯ï¼š å¦‚æœAå’ŒBç»„åˆï¼Œé‚£ä¹ˆBå’ŒCå°±ä¸èƒ½ç»„åˆ(è¿™ä¸ªé€»è¾‘æœ‰ç‚¹é—®é¢˜)ï¼Œä¾‹å¦‚ï¼š`è”¡_è‹±æ–‡` å‡ºç°ï¼Œé‚£ä¹ˆ `è‹±æ–‡_ä¹Ÿ` è¿™ä¸ªä¸æ˜¯æ–°è¯
                # ç–‘æƒ‘: **åé¢çš„é€»è¾‘ï¼Œè¿™ä¸ªæ˜¯å®Œå…¨å¯èƒ½å‡ºç°ï¼Œæ¯•ç«Ÿæ²¡æœ‰é‡å¤**
                if d[0].split('_')[-1] == pre or "".join(tmp.split('_')) in "".join(d[0].split('_')):
                    flag = False
                    break
            if flag:
                new_word = "".join(d[0].split('_'))
                add_word[new_word] = d[1]
                dict_list.append(d[0])
        b = list(map(lambda x: [x[0], x[0].replace('_', ''), x[1]], result))
        df = pd.DataFrame(b, columns=['ngramåç§°', 'åˆå¹¶åç§°', 'åˆ†å€¼'])
        # df.to_excel(newWord_path, index=0)
        return result, add_word


if __name__ == '__main__':
    pass
```


### statistic_analysis

* ç»Ÿè®¡æ–‡æœ¬é•¿åº¦,è¯é¢‘
* ä½¿ç”¨gensimè®¡ç®—tfidf
* ä½¿ç”¨sklearnè®¡ç®—tfidf
* è®¡ç®—pmi
* åŸºäºå­—é¢çš„æ¨¡ç³ŠåŒ¹é…
* åŸºäºhammingè·ç¦»çš„ç»Ÿè®¡å»é‡


```python
# -*- coding:utf-8 -*-
'''
---------
# ç›®æ ‡ä»»åŠ¡ :   æ•°æ®ç»Ÿè®¡åˆ†æ
---------------------------------
'''
import sys
sys.path.append('/home/Algorithm_Frame/CustomModule/large_scale_text_clustering')
import os
import pandas as pd
import matplotlib.pyplot as plt
import jieba.posseg as pseg
from gensim import corpora, models, matutils
# import jieba
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from conf.config import *
from core.utils import *
import logging
import pickle
import json
import Levenshtein
from thefuzz.fuzz import QRatio
from thefuzz import process
from itertools import combinations, product

logging.getLogger().setLevel(logging.WARNING)

'''ç»Ÿè®¡æ–‡æœ¬é•¿åº¦,è¯é¢‘'''
class text_statistic():
    def __init__(self,filename):
        self.filename = filename
        self.df_data = self.read_tsv()
        self.stop_words = self.read_stop()
    def read_tsv(self):
        df_data = pd.read_csv(os.path.join(dataset_dir,self.filename),sep='\t')
        logging.info('æ–‡ä»¶è¯»å–å®Œæˆ')
        return df_data

    def get_len_res(self):
        '''ç»Ÿè®¡é•¿åº¦'''
        self.df_data['text_len'] = self.df_data['patrol_desc'].str.len()
        logging.info('æ•°æ®é‡%d'%len(self.df_data['text_len']))
        self.df_data = self.df_data.loc[self.df_data['text_len']<80].copy()
        logging.info('ä¿ç•™é¢‘æ•°å¤šçš„æ•°æ®,æ•°æ®é‡%d'%len(self.df_data['text_len']))
        # print(self.df_data['text_len'].value_counts())
        self.df_data.text_len.plot(kind='hist',bins=100,color="steelblue",label="ç›´æ–¹å›¾")
        # self.df_data.text_len.plot(kind='kde',color="red",label="æ ¸å¯†åº¦å›¾")
        plt.xlabel('æ–‡æœ¬é•¿åº¦')
        plt.title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾')
        plt.legend()
        plt.savefig(os.path.join(images_dir,self.filename.replace('.tsv','.png')))
        plt.show()
        logging.info('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒåˆ†æå®Œæˆ')

    def read_stop(self):
        '''åŠ è½½åœç”¨è¯'''
        stop_words = [i.strip() for i in open(os.path.join(dataset_dir,'hit_stopwords.txt')).readlines()]
        logging.info('åœç”¨è¯åŠ è½½å®Œæˆ')
        return set(stop_words)

    def jieba_seg(self,sentence_list):
        '''ä½¿ç”¨ç»“å·´åˆ†è¯,åŠ è½½è¯æ€§'''
        # jieba.enable_parallel(10)
        pos_list = set(['n','nz','a','v','vd','vn'])
        total_list = []
        # length = len(sentence_list)
        # batch_size = 32
        # for i in range(0,length,batch_size):
        #     start = i
        #     end = i+batch_size
        #     words_list = pseg.cut(sentence_list[start:end])
        #     print(words_list)
        #     for words in words_list:
        #         print(words)
        #         word_list = []
        #         for word, flag in words:
        #             if flag in pos_list and word not in self.stop_words and len(word) > 1:
        #                 word_list.append(word)
        #         total_list.append(word_list)
        for text in tqdm(sentence_list,desc='jiebaåˆ†è¯ä¸­'):
            words = pseg.cut(text)
            word_list = []
            for word,flag in words:
                if flag in pos_list and word not in self.stop_words and len(word)>1:
                    word_list.append(word)
            total_list.append(word_list)
        # jieba.disable_parallel()
        logging.info('jiebaåˆ†è¯å®Œæˆ')
        return total_list

    def generate_wordlist(self,mode='jieba'):
        '''ç”Ÿæˆåˆ†è¯åˆ—è¡¨'''
        self.df_data['text_len'] = self.df_data['patrol_desc'].str.len()
        self.df_data = self.df_data.loc[self.df_data['text_len']<80].copy()
        sentence_list = self.df_data['patrol_desc'].values.tolist()
        if mode == 'jieba':
            total_list = self.jieba_seg(sentence_list)
        with open(os.path.join(dataset_dir,self.filename.replace('.tsv','åˆ†è¯åˆ—è¡¨.txt')),'w') as fw:
            for word_list in total_list:
                fw.write(str(word_list)+'\n')
        logging.info('åˆ†è¯è¯è¡¨ç”Ÿæˆå®Œæˆ')

'''ä½¿ç”¨gensimè®¡ç®—tfidf'''
class gensim_tfidf():
    '''https://blog.csdn.net/iszhuangsha/article/details/85163685'''
    def __init__(self,seg_filename):
        self.filename = seg_filename
        self.words_list = self.read_seg()
    def read_seg(self):
        # sklearn ç©ºæ ¼æˆ–å…¶ä»–ç¬¦å·åˆ†å‰²
        words_list = [' '.join(eval(i)) for i in open(os.path.join(dataset_dir,self.filename)).readlines()]
        return words_list
    def get_tfidf(self):
        gensim_tfidf_path = os.path.join(dataset_dir,self.filename.replace('.txt','-gensim-tfidf.mm'))
        gensim_id_token_path = os.path.join(dataset_dir,self.filename.replace('.txt','-gensim-dictionary.json'))
        if os.path.exists(gensim_tfidf_path):
            dictionary = json.load(gensim_id_token_path)
            tfidf = corpora.MmCorpus(gensim_tfidf_path)
            return tfidf,dictionary
        else:
            dictionary = corpora.Dictionary(self.words_list)
            logging.info('gensimç»Ÿè®¡è¯å…¸æ•°é‡%d' % len(dictionary))
            corpus = [dictionary.doc2bow(text) for text in self.words_list]
            tfidf = models.TfidfModel(corpus)  # å¾—åˆ°å•è¯çš„tf-idfå€¼
            with open(gensim_id_token_path,'w') as fw:
                json.dump(dictionary.id2token,fw,indent=2,ensure_ascii=False)
            corpora.MmCorpus.serialize(gensim_tfidf_path,tfidf)
            return tfidf,dictionary

'''ä½¿ç”¨sklearnè®¡ç®—tfidf'''
class sklearn_tfidf():
    def __init__(self,seg_filename):
        self.filename = seg_filename
        self.words_list = self.read_seg()
    def read_seg(self):
        # sklearn ç©ºæ ¼æˆ–å…¶ä»–ç¬¦å·åˆ†å‰²
        words_list = [' '.join(eval(i)) for i in open(os.path.join(dataset_dir,self.filename)).readlines()]
        return words_list
    def get_tfidf(self):
        '''è·å–idf:å‡ºç°è¯¥è¯çš„è¡Œæ•°/æ€»è¡Œæ•°,è¯é¢‘é»˜è®¤ä¸€è¡Œä¸­æ‰€æœ‰è¯éƒ½åªå‡ºç°ä¸€æ¬¡(çŸ­æ–‡æœ¬),æœ‰logå€¼'''

        sklearn_idf_path = os.path.join(dataset_dir,self.filename).replace('.txt','-sklearn-idf.tsv')
        sklearn_tfidf_path = os.path.join(dataset_dir,self.filename.replace('.txt','-sklearn-tfidf.pkl'))
        sklearn_vocabulary_path = os.path.join(dataset_dir,self.filename.replace('.txt','-sklearn-vocab.pkl'))

        if os.path.exists(sklearn_idf_path):
            logging.info('åŠ è½½å·²æœ‰sklearnæ¨¡å‹')
            tfidf = pickle.load(open(sklearn_tfidf_path,'rb'))
            vocab = pickle.load(open(sklearn_vocabulary_path,'rb'))
            logging.info('sklearnç»Ÿè®¡tfidfç»´åº¦ %s' % (str(tfidf.shape)))
            return tfidf,vocab
        else:
            vectorizer = CountVectorizer(decode_error="replace")
            textvector = vectorizer.fit_transform(self.words_list)
            transformer = TfidfTransformer(norm=None)
            tfidf = transformer.fit_transform(textvector)  # å‡è®¾idfå³ä¸ºæ¯ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡
            with open(sklearn_vocabulary_path,'wb') as fw:
                pickle.dump(vectorizer.vocabulary_,fw)
            with open(sklearn_tfidf_path,'wb') as fw:
                pickle.dump(sklearn_tfidf_path,fw)
            df_word_idf = pd.DataFrame(list(zip(vectorizer.get_feature_names(), transformer.idf_)), columns=['å•è¯', 'idf'])
            df_word_idf.to_csv(sklearn_idf_path,sep='\t',index=False)
            logging.info('sklearnç»Ÿè®¡tfidfç»´åº¦ %s' % (str(tfidf.shape)))
            return tfidf,vectorizer.vocabulary_

'''è®¡ç®—pmi'''
def get_pmi(seg_filename,res_file,filename):
    '''è®¡ç®—ç‚¹äº’ä¿¡æ¯'''
    from core.model import TrieNode
    root_name = os.path.join(db_dir,'root.pkl')
    stopwords = get_stopwords(os.path.join(db_dir,'hit_stopwords.txt'))
    if os.path.exists(root_name):  # å¦‚æœæœ‰æ–°çš„è¯­æ–™ä¸€å®šè¦åˆ é™¤root.pkl
        root = load_model(root_name)
    else:
        dict_name = os.path.join(db_dir,'dict_new.txt')
        word_freq = load_dictionary(dict_name)
        root = TrieNode('*', word_freq)
        save_model(root, root_name)

    # åŠ è½½æ–°çš„æ–‡ç« 
    if os.path.exists(seg_filename):
        data = [eval(i) for i in open(seg_filename).readlines()][:100]
        # print(data)
        # å°†æ–°çš„æ–‡ç« æ’å…¥åˆ°Rootä¸­
        print('------> æ’å…¥èŠ‚ç‚¹')
        for word_list in tqdm(data):
            # tmp è¡¨ç¤ºæ¯ä¸€è¡Œè‡ªç”±ç»„åˆåçš„ç»“æœï¼ˆn gramï¼‰
            # tmp: [['å®ƒ'], ['æ˜¯'], ['å°'], ['ç‹—'], ['å®ƒ', 'æ˜¯'], ['æ˜¯', 'å°'], ['å°', 'ç‹—'], ['å®ƒ', 'æ˜¯', 'å°'], ['æ˜¯', 'å°', 'ç‹—']]
            ngrams = generate_ngram(word_list, 2)
            for d in ngrams:
                root.add(d)
        print('------> æ’å…¥æˆåŠŸ')
        result = root.find_pmi()
        fw = open(res_file, 'w')
        json.dump(result, fw, indent=2, ensure_ascii=False)
        fw.close()
        # print(json.dumps(result,indent=2,ensure_ascii=False))
    else:
        # è¯­æ–™ä¸€å®šè¦æ¸…æ™°ï¼Œå»é™¤ä¹±ç ã€ç©ºç™½ç¬¦ç­‰
        # data = load_data(filename, stopwords)
        df_total,df_mapping = get_mapping_data(filename,stopwords,2)
        print('------> æ’å…¥èŠ‚ç‚¹')
        for ngrams in tqdm(df_total['n_gramåˆ—è¡¨'].values.tolist()):
            for d in ngrams:
                root.add(d)
        print('------> æ’å…¥æˆåŠŸ')
        result = root.find_pmi()
        print('------> pmiè®¡ç®—å®Œæˆ')
        fw = open(res_file, 'w')
        json.dump(result, fw, indent=2, ensure_ascii=False)
        fw.close()
        # print(json.dumps(result,indent=2,ensure_ascii=False))
        # print(df_mapping)
        # print(result)
        res_list = []
        for ind,raw in df_mapping.iterrows():
            raw = dict(raw)
            ngram = raw['n_gram']
            if len(ngram) > 1:
                ngram = '_'.join(ngram)
            if ngram in result:
                pmi = result[ngram]
            else:
                pmi = 1
            raw['pmi'] = pmi
            res_list.append(list(raw.values()))
        df_data = pd.DataFrame(res_list,columns=['n_gram','åŸå¥','pmi'])

        # æ ¹æ®pmiæ’åº
        df_data.sort_values(by='pmi',inplace=True,ascending=False)
        # ä¿ç•™ç¬¬ä¸€ä¸ªéé‡å¤å€¼
        df_data.drop_duplicates(subset=['åŸå¥'],inplace=True)
        df_data.to_excel(filename.replace('.tsv','_å…³é”®è¯ç»„åˆ.xlsx'),index=False)

        # åç»­å¯ä»¥æ ¹æ®æå–åŒ…å«æƒ³è¦çš„å…³é”®è¯çš„å¥å­,ä»¥n_gramè¿›è¡Œèšåˆ

        # æ ¹æ®é«˜é¢‘è¯ç»„åˆ

'''åŸºäºå­—é¢çš„æ¨¡ç³ŠåŒ¹é…'''
def fuzz_similarity(df_data:pd.DataFrame,field_name):
    '''
    :param df_data: å»é‡DataFrame
    :param field_name:  å»é‡å­—æ®µ
    :return: è¿”å›DataFrame
    '''
    # æ•°é‡ä¸è¶…è¿‡1000æ•ˆç‡è¾ƒé«˜

    def compute_similarity(i, j, scorer=QRatio):
        to_be_match_dict = j[j != i].to_dict()  # ä¸åŒ…å«å½“å‰çš„ä»»åŠ¡åç§°i
        if len(to_be_match_dict) == 0:
            return ['', '', '']
        res = process.extractOne(i, to_be_match_dict, scorer=scorer)  # è¿”å›ä¸‰åˆ—
        return res
    df_data.drop_duplicates(subset=[field_name],inplace=True)
    df_data = df_data.sample(frac=1)
    df_match = df_data.copy(deep=True)
    # tqdm.pandas(desc='fuzzåŒ¹é…å¤„ç†ä¸­') progress_apply
    df_match['match'] = df_data[field_name].apply(compute_similarity, args=(df_data[field_name],))
    df_match['match_'+field_name] = df_match['match'].apply(lambda x: x[0])
    df_match['match_åˆ†æ•°'] = df_match['match'].apply(lambda x: x[1])
    df_match.drop(['match'], axis=1,inplace=True)

    total_set = set()
    temp_set = set()
    for ind, raw in df_match.iterrows():
        score = raw['match_åˆ†æ•°']
        text_a = raw[field_name]
        text_b = raw['match_'+field_name]
        if score > 85:  # è®¾ç½®çš„é˜ˆå€¼
            if text_a not in temp_set:
                total_set.add(text_a)  # å…ˆæŠŠå½“å‰çš„æ•°æ®æ”¾åˆ°ç»“æœé›†ä¸­
                temp_set.add(text_a)  # å°†å½“å‰æ•°æ®æ”¾åˆ°ç¼“å­˜ä¸­
                temp_set.add(text_b)  # å°†å·²ç»åŒ¹é…è¿‡çš„æ•°æ®æ”¾åˆ°ç¼“å­˜ä¸­
        else:  # ç›¸ä¼¼åº¦ä¸é«˜
            if text_a not in temp_set:  # å½“å‰æ•°æ®ä¸åœ¨ç¼“å­˜æ•°æ®é›†é‡Œ
                total_set.add(text_a)  # æ·»åŠ åˆ°ç»“æœé›†
    df_data_1 = pd.DataFrame(total_set, columns=[field_name])
    print('æœ€ç»ˆæ•°é‡', len(total_set))
    df_match = pd.merge(df_data_1,df_match,on=field_name)

    return df_match

def hamming(a,b):
    '''len(a)==len(b)'''
    return Levenshtein.hamming(a,b)

'''åŸºäºhammingè·ç¦»çš„ç»Ÿè®¡å»é‡'''
def remove_hamming(df_data:pd.DataFrame,field_name):
    '''
    :param df_data: å¾…å»é‡çš„DataFrame
    :param filed_name: å¾…å»é‡çš„å­—æ®µ
    :return: è¿”å›å»é‡åçš„DataFrame
    '''
    # æ•°é‡ä¸è¶…è¿‡10000æ•ˆç‡è¾ƒé«˜
    df_match = df_data.copy(deep=True)
    df_match = df_match[[field_name]]  # è¦å»é‡çš„å­—æ®µ
    df_match.drop_duplicates(inplace=True)
    df_match['text_len'] = df_match[field_name].str.len()  # è®¡ç®—æ–‡æœ¬é•¿åº¦
    df_match = df_match.loc[df_match['text_len'] > 5].copy()
    print('æ•°æ®é‡', len(df_match))  # 333634
    df_match = df_match.sample(frac=1, random_state=22)   # éšæœºé€‰æ‹©ä¸€å®šæ¯”ä¾‹æ•°æ®
    total_list = []
    for df_data_single in tqdm(df_match.groupby('text_len'), desc='df_match'):  # æŒ‰ä¸åŒé•¿åº¦è¿›è¡Œåˆ†ç»„
        text_list = df_data_single[1][field_name].values.tolist()
        num = 0
        while len(text_list) != 1 and len(text_list) > num + 1:
            a = [text_list[num]]
            b = text_list[num + 1:]
            c = product(a, b)
            for i in c:
                score = hamming(i[0], i[1])
                if score <= 2 or score > 10:  # è®¾ç½®è¿‡æ»¤å€¼å¤§å°
                    text_list.remove(i[1])
            num += 1
        total_list.extend(text_list)
    total_set = set(total_list)
    df_match.drop(columns=['text_len'], inplace=True)
    total_list = []
    for ind, raw in tqdm(df_match.iterrows()):
        raw = dict(raw)
        text_a = raw[field_name]
        if text_a in total_set:
            total_list.append(list(raw.values()))
    print('df_match å»é‡å‰æ•°é‡', len(df_match))
    print('df_match æ±‰æ˜å»é‡åæ•°é‡', len(total_list))
    df = pd.DataFrame(total_list, columns=df_match.columns.tolist())

    df_match = pd.merge(df,df_data,on=field_name)
    return df_match
    # df.to_excel(os.path.join('v7', 'å¾…æ¸…æ´—å»é‡æ•°æ®', '0524-%d-å¾…æ ‡æ³¨.xlsx' % len(total_list)), index=False)

if __name__ == '__main__':
    # text_sta = text_statistic('æ•°æ®.tsv')
    # text_sta.get_len_res()  # æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
    # text_sta.generate_wordlist()

    # sklearn_tf_idf = sklearn_tfidf('æ•°æ®åˆ†è¯åˆ—è¡¨.txt')
    # sklearn_tf_idf.get_tfidf()


    # ç¼–è¾‘è·ç¦»è¿‡æ»¤
    df_data = pd.DataFrame({"ä»»åŠ¡åç§°":["è¿™æ˜¯ä¸€ä¸ªä»»åŠ¡åç§°","è¿™æ˜¯ä¸¤ä¸ªä»»åŠ¡åç§°çš„è®¡ç®—",'è¿™æ˜¯ä¸¤ä¸ªä»»åŠ¡åç§°'],
                            "ä¸´æ—¶å­—æ®µ":["æ‡‚æ³•å®ˆæ³•","é‡‘ä½›æ–‡é£çš„åˆ†é…","é‡‘ä½›æ–‡é£çš„åˆ†"],
                            "ä¸´æ—¶": ["dsofdhsfds",'dfosdhfdsfo','dshfodsh']})
    # print(df_data.head())
    # df = fuzz_similarity(df_data,"ä»»åŠ¡åç§°")
    # df = remove_hamming(df_data,"ä»»åŠ¡åç§°")
    # print(df.head())

    # get_pmi(os.path.join(dataset_dir,'æ•°æ®åˆ†è¯åˆ—è¡¨.txt'),
    #         os.path.join(dataset_dir,'æ•°æ®PMIç»“æœ.json'),
    #         '')
    get_pmi('',
            os.path.join(dataset_dir,'æ•°æ®PMIç»“æœ.json'),
            os.path.join(dataset_dir,'æ•°æ®.tsv'))
    #
    pass
```


### text_representation

* hashè¡¨ç¤º
* é€šè¿‡PTM-encodeæ–‡æœ¬å‘é‡
* å¤šè¿›ç¨‹ç”Ÿæˆå¤§æ‰¹é‡æ•°æ®


```python
# -*- coding:utf-8 -*-

from conf.config import *

from simhash import Simhash
from tqdm import tqdm
import torch
import pickle
from transformers import AlbertModel, BertTokenizer
import pandas as pd
import logging
import re

def filter_puntuation(string):
    text=re.sub('[\.\!\/_,-:;<>â‰¦$%^*()+\"\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+',' ',string)
    text=re.sub('  ','',text,3)
    return text

def get_features(s,width):
    s = s.lower()
    s = re.sub(r'[^\w]+', '', s)
    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]

def read_tsv(text_path):
    df_data = pd.read_csv(text_path, sep='\t')
    df_data.rename(columns={'patrol_desc': 'text_a'}, inplace=True)
    return df_data

'''hashè¡¨ç¤º'''
def hash_vector(filename,seg_file,ngram=2,use_seg=True):
    logging.getLogger("simhash").setLevel(logging.WARNING)
    file_path = os.path.join(dataset_dir, filename)
    df_data = read_tsv(file_path)
    out_file = os.path.join(dataset_dir, filename).replace('.tsv', '-hash.tsv')
    text_list = df_data['text_a'].values.tolist()
    logging.info('æ•°æ®æ€»é•¿åº¦ %d'%len(text_list))
    total_list = []
    if use_seg: # è¯»å–åˆ†è¯åˆ—è¡¨
        seg_file = os.path.join(dataset_dir,seg_file)
        fr = open(seg_file,'r').readlines()
        for id,seg,in tqdm(enumerate(fr),desc='åˆ†è¯hashå¤„ç†'):
            simhash_ = Simhash(eval(seg)).value
            total_list.append([id,text_list[id],simhash_])
    else:
        for id,text in tqdm(enumerate(text_list),desc='åŸå¥hashå¤„ç†'):
            simhash_ = Simhash(get_features(text,ngram)).value
            total_list.append([id,text,simhash_])
    df = pd.DataFrame(total_list,columns=['id','text','simhash'])
    df.to_csv(out_file,sep='\t',index=False)
    logging.info('simhashå‘é‡è¡¨å¾å®Œæˆ')
    return df


'''é€šè¿‡PTM-encodeæ–‡æœ¬å‘é‡'''
def albert_create_vector(filename):
    '''ä½¿ç”¨è‡ªå®šä¹‰çš„é¢„è®­ç»ƒæ¨¡å‹'''
    pretrained_path = os.path.join(ptm_dir, 'PretrainingModel/tf-albert')
    file_path = os.path.join(dataset_dir,filename)
    corpus_vector_pkl_path = os.path.join(dataset_dir, filename.replace('.tsv','-vector.pkl'))

    os.environ["CUDA_VISIBLE_DEVICES"] = '1'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = BertTokenizer.from_pretrained(pretrained_path)
    model = AlbertModel.from_pretrained(pretrained_path, from_tf=True).to(device)
    logging.info('albertæ­£å¸¸åŠ è½½æ¨¡å‹')
    df_data = read_tsv(file_path)
    text_list = df_data['text_a'].values.tolist()
    total_length = len(text_list)
    batchsize = 128
    max_length = 64
    vectors = []
    for i in tqdm(range(0,total_length,batchsize),desc='æå–å¥å‘é‡'):
        start = i
        end = i+batchsize
        input_id_list = []
        for inputtext in text_list[start:end]:
            input_ids = tokenizer.encode(inputtext)
            input_id_list.append(input_ids)
        input_ids_padding = []
        for i in input_id_list:
            if max_length > len(i):
                input_ids_padding.append(i + [0] * (max_length - len(i)))
            else:
                input_ids_padding.append(i[:max_length])
        input_ids_padding = torch.tensor(input_ids_padding).to(device)
        with torch.no_grad():
            vector = model(input_ids_padding).pooler_output.cpu().detach().numpy()
            vectors.extend(vector)
    embeddign_cache = open(corpus_vector_pkl_path,'wb')
    pickle.dump({"vectors":vectors},embeddign_cache)
    embeddign_cache.close()
    logging.info('albertæå–å¥å‘é‡å®Œæˆ,æ•°é‡ %d'%len(vectors))


'''å¤šè¿›ç¨‹ç”Ÿæˆå¤§æ‰¹é‡æ•°æ®'''

class multi_vector():

    def __init__(self,filename):
        self.filename = filename
        df_data = read_tsv(os.path.join(dataset_dir,filename))
        self.text_list = df_data['text_a'].values.tolist()
    def single_process(self,inputtext_list,start_index,end_index):
        pretrained_path = os.path.join(ptm_dir, 'PretrainingModel/tf-albert')

        os.environ["CUDA_VISIBLE_DEVICES"] = '0'
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        tokenizer = BertTokenizer.from_pretrained(pretrained_path)
        model = AlbertModel.from_pretrained(pretrained_path, from_tf=True).to(device)
        inputtext_list = inputtext_list[start_index:end_index]
        total_length = len(inputtext_list)
        batchsize = 128
        max_length = 64
        vectors = []
        for i in tqdm(range(0, total_length, batchsize), desc='æå–å¥å‘é‡%d-%d' % (start_index, end_index)):
            start = i
            end = i + batchsize
            input_id_list = []
            for inputtext in inputtext_list[start:end]:
                input_ids = tokenizer.encode(inputtext)
                input_id_list.append(input_ids)
            input_ids_padding = []
            for i in input_id_list:
                if max_length > len(i):
                    input_ids_padding.append(i + [0] * (max_length - len(i)))
                else:
                    input_ids_padding.append(i[:max_length])
            input_ids_padding = torch.tensor(input_ids_padding).to(device)
            with torch.no_grad():
                vector = model(input_ids_padding).pooler_output.cpu().detach().numpy()
                vectors.extend(vector)
        return vectors
    def multi_process(self):
        import multiprocessing as mp
        from multiprocessing import Pool
        process_num = 5
        Pool(processes=process_num)
        total_length = len(self.text_list)
        # æ ¹æ®è¿›ç¨‹æ•°è®¾ç½®èµ·å§‹ç»“æŸç´¢å¼•å€¼
        per_num = total_length // process_num
        results = []
        for i in range(process_num):
            start = per_num * i
            end = per_num * (i + 1)
            res = mp.Process(target=self.single_process, args=(self.text_list, start, end))
            results.append(res)
            res.start()
        for task in results:
            task.join()
        vectors = []
        for i in results:
            vectors.extend(i.get())
        corpus_vector_pkl_path = os.path.join(dataset_dir, self.filename.replace('.tsv', '-vector.pkl'))
        embeddign_cache = open(corpus_vector_pkl_path, 'wb')
        pickle.dump({"vectors": vectors}, embeddign_cache)
        embeddign_cache.close()
        logging.info('å¤šè¿›ç¨‹æå–å¥å‘é‡å®Œæˆ,æ•°é‡ %d' % len(vectors))


if __name__ == '__main__':

    # print(get_features('4å·æ¥¼é—¨',2))
    hash_vector('æ•°æ®.tsv','',use_seg=False)

    pass
```



### utils

* åœç”¨è¯åŠ è½½
* n_gramç”Ÿæˆå™¨
* åŠ è½½è‡ªå®šä¹‰è¯å…¸æ„é€ èŠ‚ç‚¹æ ‘
* ç”ŸæˆDataFrameæ ¼å¼,åŸå¥,åˆ†è¯,n_gramæ•°æ®


```python
# -*- coding:utf-8 -*-

import logging
import pickle
from tqdm import tqdm
import jieba
import pandas as pd
import os
import jieba.posseg as pseg

def get_stopwords(stop_path):
    with open(stop_path, 'r',encoding='utf-8') as f:
        stopword = [line.strip() for line in f]
    return set(stopword)


def generate_ngram(input_list, n):
    '''
    :param input_list: è¾“å…¥ ['æœºæ„', 'ç»“æ„', 'æ¡†']
    :param n: n_gram
    :return: [('æœºæ„',), ('ç»“æ„',), ('æ¡†',), ('æœºæ„', 'ç»“æ„'), ('ç»“æ„', 'æ¡†')]
    '''
    result = []
    for i in range(1, n+1):
        result.extend(zip(*[input_list[j:] for j in range(i)]))
    # logging.info('ngramæ•°æ®ç”Ÿæˆå®Œæˆ')
    return result

def load_dictionary(filename):
    """
    åŠ è½½å¤–éƒ¨è¯é¢‘è®°å½•,æœ€å¥½æ˜¯å‚ç›´ä¸šåŠ¡è¯å…¸
    :param filename:
    :return:
    """
    word_freq = {}
    with open(filename, 'r',encoding='utf-8') as f:
        for line in f:
            try:
                line_list = line.strip().split(' ')
                # è§„å®šæœ€å°‘è¯é¢‘
                if int(line_list[1]) >= 1:
                    word_freq[line_list[0]] = line_list[1]
            except IndexError as e:
                print(line)
                continue
    return word_freq

def save_model(model, filename):
    with open(filename, 'wb') as fw:
        pickle.dump(model, fw)


def load_model(filename):
    with open(filename, 'rb') as fr:
        model = pickle.load(fr)
    return model


def load_data(filename, stopwords):
    """
    :param filename:
    :param stopwords:
    :return: äºŒç»´æ•°ç»„,[[å¥å­1åˆ†è¯list], [å¥å­2åˆ†è¯list],...,[å¥å­nåˆ†è¯list]]
    """
    data = []
    with open(filename, 'r',encoding='utf-8') as f:
        for line in f:
            line_list = line.strip().split(' ')
            for i in line_list:
                word_list = [x for x in jieba.cut(i, cut_all=False) if x not in stopwords]
                data.append(word_list)
    return data


# ç”ŸæˆDataFrameæ•°æ®
def get_mapping_data(filename,stopwords,n):
    '''
    filename: tsvæ•°æ®(å…¶ä»–æ•°æ®æ ¼å¼è‡ªå®šä¹‰å¤„ç†)
    stopwords: åœç”¨è¯é›†åˆ
    :return: æ–‡ä»¶1 ['åŸå¥','åˆ†è¯åˆ—è¡¨','n_gramåˆ—è¡¨'] æ–‡ä»¶2['n_gram','åŸå¥']
    '''


    df_data = pd.read_csv(filename, sep='\t',nrows=10000)
    sentence_list = df_data['patrol_desc'].values.tolist()  # è‡ªå®šä¹‰å­—æ®µ

    pos_list = set(['n','nz','a','v','vd','vn'])
    total_list = []
    map_list = []
    for ind,text in tqdm(enumerate(sentence_list),desc='jiebaåˆ†è¯ä¸­'):
        words = pseg.cut(text)
        word_list = []
        for word,flag in words:
            if flag in pos_list and word not in stopwords and len(word)>1:
                word_list.append(word)
        n_gram_list = generate_ngram(word_list,n)
        total_list.append([text,word_list,n_gram_list])
        for i in n_gram_list:
            map_list.append([i,text])

    df_total = pd.DataFrame(total_list,columns=['åŸå¥','åˆ†è¯åˆ—è¡¨','n_gramåˆ—è¡¨'])
    df_total.to_excel(filename.replace('.tsv','_åˆ†è¯_ngram.xlsx'),index=False)

    df_mapping = pd.DataFrame(map_list,columns=['n_gram','åŸå¥'])
    df_mapping.to_excel(filename.replace('.tsv','_ngramæ˜ å°„.xlsx'))

    # jieba.disable_parallel()
    logging.info('jiebaåˆ†è¯å®Œæˆ')
    return df_total,df_mapping

if __name__ == '__main__':
    pass
```

## è¡¥å……

### faiss, milvuså‘é‡èšç±»åŸç†

* èƒŒæ™¯
* ä¸€ä¸ªç®€å•çš„ç‰¹å¾ç›¸ä¼¼åº¦æ¯”å¯¹çš„ä¾‹å­
* åœ¨å¤šä¸ªCUDAè®¾å¤‡ä¸Šè¿›è¡Œç‰¹å¾çš„ç›¸ä¼¼åº¦æœç´¢
* Faissç¯å¢ƒå‡†å¤‡
* Faissçš„ç®€å•ä½¿ç”¨ï¼šFlat
* 
> <font color=#FF000 >xbç”¨æ¥è¡¨ç¤ºç‰¹å¾åº“ï¼Œxqç”¨æ¥è¡¨ç¤ºå¾…æŸ¥è¯¢çš„2048ç»´å‘é‡</font>ã€‚å¦‚æœæ²¿ç”¨ä¸Šé¢çš„ä¾‹å­ï¼Œåˆ™xbå°±æ˜¯æå‰å­˜å‚¨äº†7030ä¸ªæ ·æœ¬çš„ç‰¹å¾çš„â€œæ•°æ®åº“â€ï¼Œå®ƒçš„shapeå°±æ˜¯7030x2048â€”â€”è¿™æ ·çš„<font color=#FF000 >â€œæ•°æ®åº“â€åœ¨Faissä¸­ç§°ä½œIndex object</font>

> IndexFlatL2:æœ€ç®€å•çš„åº“:ä½¿ç”¨æš´åŠ›L2æœç´¢çš„æ•°æ®åº“â€”â€”ä¹Ÿå°±æ˜¯å’Œç‰¹å¾åº“ä¸­çš„æ¯ä¸ªç‰¹å¾è¿›è¡ŒL2è·ç¦»è®¡ç®—ç„¶åå–å‡ºè·ç¦»æœ€è¿‘çš„é‚£ä¸ª
Index objectçš„â€œè®­ç»ƒâ€å…¶å®å°±æ˜¯<font color=#FF000 >æå‰æ ¹æ®ç‰¹å¾çš„åˆ†å¸ƒè¿›è¡Œèšç±»è®­ç»ƒ</font>,IndexFlatL2å¹¶ä¸åœ¨åˆ—
xqçš„batchå¾ˆå°ï¼ŒIndexå¾ˆå°ï¼šCPUé€šå¸¸æ›´å¿«ï¼›
xqçš„batchå¾ˆå°ï¼ŒIndexå¾ˆå¤§ï¼šGPUé€šå¸¸æ›´å¿«ï¼›
xqçš„batchå¾ˆå¤§ï¼ŒIndexå¾ˆå°ï¼šéšä¾¿ï¼›
xqçš„batchå¾ˆå¤§ï¼ŒIndexå¾ˆå¤§ï¼šGPUé€šå¸¸æ›´å¿«ï¼›
GPUé€šå¸¸æ¯”CPUå¿«5åˆ°10å€ï¼›

* è®©Faissä½¿ç”¨æ›´å°‘çš„å†…å­˜ï¼šPQ
> Product Quantizer,è¿™æ˜¯ä¸€ç§æœ‰æŸå‹ç¼©ï¼Œæ‰€ä»¥è¿™ç§Indexè¿›è¡Œæ£€ç´¢çš„è¿”å›å€¼åªæ˜¯è¿‘ä¼¼çš„å‡†ç¡®
ä¸Šé¢çš„centroidï¼ˆæœ€ç±»ä¸­å¿ƒçš„é‚£ä¸ª2048ç»´å‘é‡ï¼‰ï¼Œåœ¨Faissä¸­æˆ‘ä»¬ç§°ä¹‹ä¸ºcodeï¼›ä¸Šè¿°çš„8ç»„256ä¸ªcentroidï¼Œåœ¨Faissä¸­æˆ‘ä»¬ç§°ä¹‹ä¸º8ä¸ªcode bookï¼›è¿™8ä¸ªcode bookå¯ä»¥è¡¨è¾¾256^8ä¸ªå€¼ï¼Œè¿˜æ˜¯å¾ˆå¤§çš„ã€‚

* è®©Faissè¿›è¡Œæ›´å¿«çš„æ£€ç´¢ï¼šIVF

> ä¸¤ä¸¤ç‰¹å¾æ¯”å¯¹æ›´å°‘çš„è®¡ç®—é‡ï¼›PQé¡ºå¸¦ç€åšäº†ï¼›
åªå’Œç‰¹å¾åº“çš„ä¸€éƒ¨åˆ†è¿›è¡Œæ¯”å¯¹ï¼›å’Œç‰¹å¾åº“çš„æ¯ä¸€ä¸ªç‰¹å¾è¿›è¡Œæ¯”å¯¹ï¼Œå«åšç©·ä¸¾ï¼›åªå’Œéƒ¨åˆ†ç‰¹å¾è¿›è¡Œæ¯”å¯¹ï¼Œå«åšIVF(å€’æ’ç´¢å¼•)ï¼›
å› æ­¤ï¼Œåœ¨Faissä¸­æ‰€æœ‰å¸¦æœ‰IVFçš„indexï¼ŒæŒ‡çš„éƒ½æ˜¯æå‰å°†æ•°æ®åº“ä½¿ç”¨k-meansèšç±»ç®—æ³•åˆ’åˆ†ä¸ºå¤šä¸ªpartitionï¼Œæ¯ä¸ªpartitionä¸­åŒ…å«æœ‰å¯¹åº”çš„feature vectorsï¼ˆè¿™å°±æ˜¯inverted file listsæŒ‡å‘çš„ï¼‰ï¼ŒåŒæ—¶ï¼Œæ¯ä¸ªpartitionè¿˜æœ‰å¯¹åº”çš„centroidï¼Œç”¨æ¥å†³å®šè¯¥partitionæ˜¯åº”è¯¥è¢«ç©·ä¸¾æœç´¢ï¼Œè¿˜æ˜¯è¢«å¿½ç•¥ã€‚

> <font color=#FF000 >è¿™ä¸ªpartitionçš„æ¦‚å¿µï¼Œåœ¨Faissä¸­ç§°ä¹‹ä¸ºVoronoi cells</font>ï¼›é€‰æ‹©æŸä¸ªVoronoi cellsç„¶åè¿›è¡Œæ£€ç´¢çš„åŠ¨ä½œï¼Œç§°ä¹‹ä¸º<font color=#FF000 >â€œprobeâ€</font>ï¼›è€Œåœ¨æœ€è¿‘çš„â€œå¤šå°‘ä¸ªâ€Voronoi cellsä¸­è¿›è¡Œæ£€ç´¢ï¼Œè¿™ä¸ªâ€œå¤šå°‘ä¸ªâ€çš„æ•°é‡ç§°ä¹‹ä¸ºnprobeï¼›åœ¨IVF Index objectçš„å±æ€§ä¸­ï¼Œå°±æœ‰nprobeè¿™ä¸ªå±æ€§
==IndexIVFPQçš„å‚æ•°ä¸­ï¼Œdä»£è¡¨ç‰¹å¾çš„ç»´åº¦2048ï¼Œnlistä»£è¡¨Voronoi cellsçš„æ•°é‡ï¼Œmä»£è¡¨subquantizersçš„æ•°é‡ï¼ˆç»™PQç”¨ï¼‰ï¼Œ8ä»£è¡¨ä½¿ç”¨8bitsè¡¨ç¤ºä¸€ä¸ªç‰¹å¾çš„1ä¸ªsub-vector==ã€‚

* æ— è®ºå¦‚ä½•å†…å­˜éƒ½ä¸å¤Ÿç”¨ï¼šDistributed index
> å°†ç‰¹å¾åº“åˆ†æ•£åŠ è½½åˆ°å¤šä¸ªæœåŠ¡å™¨çš„CUDAè®¾å¤‡ä¸Š

* æ— è®ºå¦‚ä½•å†…å­˜éƒ½ä¸å¤Ÿç”¨ï¼šOn-disk storage
> å‚è€ƒé¡¹ç›®ä¸­çš„faiss/contrib/ondisk.py

* å½“xqæ˜¯pytorchçš„tensoræ—¶
> Faissæä¾›äº†ä¸€ç§ä¸´æ—¶çš„æªæ–½ï¼Œå¯ä»¥ç›´æ¥è¯»å–Tensoråº•å±‚çš„å†…å­˜ï¼ˆTensorå¿…é¡»æ˜¯is_contiguousçš„ï¼‰ï¼Œç„¶åä½¿ç”¨indexçš„search_c() APIæ¥æ£€ç´¢ã€‚å…³äºåœ¨indexä¸­æ£€ç´¢PyTorch Tensorçš„è¯¦ç»†ç”¨æ³•ï¼Œ

* æœ€åï¼Œå¦‚ä½•é€‰æ‹©ä¸€ç§Indexå‘¢ï¼Ÿ
> æˆ‘ä»¬å·²ç»è§è¯†è¿‡çš„å…³é”®å­—æœ‰Flatã€IVFã€PQï¼Œé‚£ä¹ˆå¦‚ä½•é€‰æ‹©ä¸€ç§Indexæ¥åŒ¹é…æˆ‘ä»¬çš„åœºæ™¯å‘¢ï¼Ÿ

1. å½“éœ€è¦ç»å¯¹å‡†ç¡®çš„ç»“æœæ—¶ï¼Œä½¿ç”¨Flatï¼›æ¯”å¦‚IndexFlatL2 æˆ–è€… IndexFlatIPï¼›
2. å¦‚æœå†…å­˜å®Œå…¨å¤Ÿç”¨å¯Œè£•çš„ä¸è¡Œï¼Œä½¿ç”¨HNSWï¼›å¦‚æœä¸€èˆ¬å¤Ÿç”¨ï¼Œä½¿ç”¨Flatï¼›å¦‚æœæœ‰ç‚¹åƒç´§ï¼Œä½¿ç”¨PCARx,...,SQ8ï¼›å¦‚æœéå¸¸åƒç´§ï¼Œä½¿ç”¨OPQx_y,...,PQxï¼›
3. å¦‚æœç‰¹å¾åº“å°äº1Mä¸ªè®°å½•ï¼Œä½¿ç”¨"...,IVFx,..."==ï¼›å¦‚æœåœ¨1Måˆ°10Mä¹‹é—´ï¼Œä½¿ç”¨"...,IVF65536_HNSW32,..."==ï¼›å¦‚æœåœ¨10M - 100Mä¹‹é—´ï¼Œä½¿ç”¨"...,IVF262144_HNSW32,..."ï¼›å¦‚æœåœ¨100M - 1Bä¹‹é—´ï¼Œä½¿ç”¨"...,IVF1048576_HNSW32,..."ã€‚

> ç”¨äºè®­ç»ƒçš„xbè®°å½•è¶Šå¤šï¼Œèšç±»ç®—æ³•çš„è®­ç»ƒæ•ˆæœè¶Šå¥½ï¼Œä½†è®­ç»ƒéœ€è¦èŠ±çš„æ—¶é—´ä¹Ÿå°±è¶Šå¤šï¼Œåˆ«å¿˜äº†è¿™ä¸€ç‚¹ã€‚


[FAISS æ•™ç¨‹](https://zhuanlan.zhihu.com/p/320653340)

IndexIVFFlat
> å…ˆèšç±»å†æœç´¢ï¼Œå¯ä»¥åŠ å¿«æ£€ç´¢é€Ÿåº¦
å…ˆå°†xbä¸­çš„æ•°æ®è¿›è¡Œèšç±»ï¼ˆèšç±»çš„æ•°ç›®æ˜¯è¶…å‚ï¼‰ï¼Œnlist: èšç±»çš„æ•°ç›®
nprobe: åœ¨å¤šå°‘ä¸ªèšç±»ä¸­è¿›è¡Œæœç´¢ï¼Œé»˜è®¤ä¸º1, nprobeè¶Šå¤§ï¼Œç»“æœè¶Šç²¾ç¡®ï¼Œä½†æ˜¯é€Ÿåº¦è¶Šæ…¢

IndexIVFPQ
> åŸºäºä¹˜ç§¯é‡åŒ–ï¼ˆproduct quantizersï¼‰å¯¹å­˜å‚¨å‘é‡è¿›è¡Œå‹ç¼©ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´
mï¼šä¹˜ç§¯é‡åŒ–ä¸­ï¼Œå°†åŸæ¥çš„å‘é‡ç»´åº¦å¹³å‡åˆ†æˆå¤šå°‘ä»½ï¼Œdå¿…é¡»ä¸ºmçš„æ•´æ•°å€
bits: æ¯ä¸ªå­å‘é‡ç”¨å¤šå°‘ä¸ªbitsè¡¨ç¤º

[Milvus ç™¾ä¸‡å‘é‡æœç´¢ï¼ˆSIFT1Bï¼‰](https://juejin.cn/post/6844904034797617159)

### éƒ¨åˆ†æ’å…¥æ£€ç´¢ä»£ç 

**faiss**

```python
def build_index(encoder_conf, index_file_name, title_list, para_list):
    dual_encoder = rocketqa.load_model(**encoder_conf)
    para_embs = dual_encoder.encode_para(para=para_list, title=title_list)
    para_embs = np.array(list(para_embs))

    print("Building index with Faiss...")
    indexer = faiss.IndexFlatIP(768)
    indexer.add(para_embs.astype('float32'))
    faiss.write_index(indexer, index_file_name)
class FaissTool():
    """
    Faiss index tools
    """
    def __init__(self, text_filename, index_filename):
        self.engine = faiss.read_index(index_filename)
        self.id2text = []
        for line in open(text_filename):
            self.id2text.append(line.strip())

    def search(self, q_embs, topk=5):
        res_dist, res_pid = self.engine.search(q_embs, topk)
        result_list = []
        for i in range(topk):
            result_list.append(self.id2text[res_pid[0][i]])
        return result_list
```

**milvus_util**

```python
import sys
sys.path.append('./')

from milvus import *
from utils.config import MILVUS_HOST, MILVUS_PORT, collection_param, index_type, index_param
from utils.config import top_k, search_param


class VecToMilvus:
    def __init__(self):
        self.client = Milvus(host=MILVUS_HOST, port=MILVUS_PORT)

    def has_collection(self, collection_name):
        try:
            status, ok = self.client.has_collection(collection_name)
            return ok
        except Exception as e:
            print("Milvus has_table error:", e)

    def creat_collection(self, collection_name):
        try:
            collection_param["collection_name"] = collection_name
            status = self.client.create_collection(collection_param)
            print(status)
            return status
        except Exception as e:
            print("Milvus create collection error:", e)

    def create_index(self, collection_name):
        try:
            status = self.client.create_index(collection_name, index_type, index_param)
            print(status)
            return status
        except Exception as e:
            print("Milvus create index error:", e)

    def has_partition(self, collection_name, partition_tag):
        try:
            status, ok = self.client.has_partition(collection_name, partition_tag)
            return ok
        except Exception as e:
            print("Milvus has partition error: ", e)

    def delete_partition(self, collection_name, partition_tag):
        try:
            status = self.client.drop_collection(collection_name)
            return status
        except Exception as e:
            print("Milvus has partition error: ", e)

    def create_partition(self, collection_name, partition_tag):
        try:
            status = self.client.create_partition(collection_name, partition_tag)
            print("create partition {} successfully".format(partition_tag))
            return status
        except Exception as e:
            print("Milvus create partition error: ", e)

    def insert(self, vectors, collection_name, ids=None, partition_tag=None):
        try:
            if not self.has_collection(collection_name):
                self.creat_collection(collection_name)
                self.create_index(collection_name)
                print("collection info: {}".format(self.client.get_collection_info(collection_name)[1]))
            if (partition_tag is not None) and (not self.has_partition(collection_name, partition_tag)):
                self.create_partition(collection_name, partition_tag)
            status, ids = self.client.insert(
                collection_name=collection_name, records=vectors, ids=ids, partition_tag=partition_tag
            )
            self.client.flush([collection_name])
            print(
                "Insert {} entities, there are {} entities after insert data.".format(
                    len(ids), self.client.count_entities(collection_name)[1]
                )
            )
            return status, ids
        except Exception as e:
            print("Milvus insert error:", e)


class RecallByMilvus:
    def __init__(self):
        self.client = Milvus(host=MILVUS_HOST, port=MILVUS_PORT)

    def search(self, vectors, collection_name, partition_tag=None):
        try:
            status, results = self.client.search(
                collection_name=collection_name,
                query_records=vectors,
                top_k=top_k,
                params=search_param,
                partition_tag=partition_tag,
            )
            return status, results
        except Exception as e:
            print("Milvus recall error: ", e)

```

**config**

```python
from milvus import MetricType, IndexType

MILVUS_HOST = "10.0.98.28"
MILVUS_PORT = 19530

output_emb_size = 256

collection_param = {
    "dimension": output_emb_size if output_emb_size > 0 else 768,
    "index_file_size": 256,
    "metric_type": MetricType.L2,
}

index_type = IndexType.FLAT
index_param = {"nlist": 1000}

top_k = 5
search_param = {"nprobe": 5}

# collection_name = "label_text"
collection_name = "serving_3_frame"
partition_tag = "partition_2"
```

**vector_insert**

```python
import argparse

from tqdm import tqdm
import numpy as np

from milvus_util import VecToMilvus
from config import collection_name, partition_tag

# yapf: disable
parser = argparse.ArgumentParser()
parser.add_argument("--vector_path", type=str, default='./data/3/label_embedding.npy',
    help="feature file path.")

args = parser.parse_args()
# yapf: enable


def vector_insert(file_path):
    embeddings = np.load(file_path)
    print(embeddings.shape)
    embedding_ids = [i for i in range(embeddings.shape[0])]
    print(len(embedding_ids))
    client = VecToMilvus()

    if client.has_partition(collection_name, partition_tag):
        client.delete_partition(collection_name, partition_tag)
    data_size = len(embedding_ids)
    batch_size = 50000
    for i in tqdm(range(0, data_size, batch_size)):
        cur_end = i + batch_size
        if cur_end > data_size:
            cur_end = data_size
        batch_emb = embeddings[np.arange(i, cur_end)]
        status, ids = client.insert(
            collection_name=collection_name,
            vectors=batch_emb.tolist(),
            ids=embedding_ids[i : i + batch_size],
            partition_tag=partition_tag,
        )


if __name__ == "__main__":
    vector_insert(args.vector_path)
```



[paddlenlp](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/text_classification/multi_class/retrieval_based/utils)

## å‚è€ƒ

* https://zhuanlan.zhihu.com/p/366254425
* https://github.com/liuhuanyong/SinglepassTextCluster
* https://github.com/zhanzecheng/Chinese_segment_augment
* [sklearnä¸­æ–‡ç¤¾åŒº](https://scikit-learn.org.cn/view/108.html)
* [èœèœsklearn](https://zhuanlan.zhihu.com/p/139945378)
* [ä¸€æ–‡å…¥é—¨Facebookå¼€æºå‘é‡æ£€ç´¢æ¡†æ¶Faiss](https://zhuanlan.zhihu.com/p/320653340)
* [ä½¿ç”¨Faissè¿›è¡Œæµ·é‡ç‰¹å¾çš„ç›¸ä¼¼åº¦åŒ¹é…](https://cloud.tencent.com/developer/article/1770696)
