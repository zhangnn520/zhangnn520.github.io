---
sort: 1
---

# æ•°æ®åˆ†å¸ƒç›¸å…³é—®é¢˜

> æŒç»­æ›´æ–°ä¸­


[ğŸ”¨ç®—æ³•å¼€å‘æ‰‹å†Œ](https://kg-nlp.github.io/Algorithm-Project-Manual/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98.html)


[ğŸ”¨ä¸ªäººçŸ¥ä¹](https://www.zhihu.com/people/zhangyj-n)




* [è¿ç§»å­¦ä¹ ä¸­ä¸ºä»€ä¹ˆè¦ç ”ç©¶è¾¹ç¼˜æ¦‚ç‡åˆ†å¸ƒå’Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ](https://www.zhihu.com/question/293820673)
    * è¿ç§»å­¦ä¹ çš„ä¸€å¤§é—®é¢˜æ˜¯é¢†åŸŸè‡ªé€‚åº”é—®é¢˜ï¼Œå³å¸Œæœ›åœ¨æœ‰å¤§é‡æ ‡æ³¨æ•°æ®è®­ç»ƒçš„æºåŸŸä¸­è®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨åªæœ‰å°‘é‡æ ‡æ³¨æˆ–è€…æ— æ ‡æ³¨æ•°æ®çš„ç›®æ ‡åŸŸä¸­æœ‰å¾ˆå¥½çš„æ³›åŒ–æ€§ã€‚å…¶ä¸­çš„æŒ‘æˆ˜åœ¨äºæºåŸŸå’Œç›®æ ‡åŸŸçš„æ•°æ®åˆ†å¸ƒå¾ˆå¯èƒ½æ˜¯ä¸åŒçš„ï¼Œå³æºåŸŸå’Œç›®æ ‡åŸŸçš„è”åˆæ¦‚ç‡åˆ†å¸ƒå¯èƒ½æ˜¯ä¸åŒçš„ã€‚  
    * ç”±è´å¶æ–¯å…¬å¼å¯å°†è”åˆæ¦‚ç‡è®¡ç®—åˆ†ä¸ºè¾¹ç¼˜æ¦‚ç‡åˆ†å¸ƒå’Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚
    * æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒç›¸åŒï¼Œä¹Ÿå°±æ˜¯å­¦ä¹ ä»»åŠ¡ç›¸åŒï¼Œä½†æºåŸŸå’Œç›®æ ‡åŸŸçš„è¾“å…¥è¾¹ç¼˜åˆ†å¸ƒå¯èƒ½ä¸åŒï¼Œæˆ–æ˜¯æ ‡ç­¾åˆ†å¸ƒå¯èƒ½ä¸åŒã€‚è¾“å…¥è¾¹ç¼˜åˆ†å¸ƒä¸åŒï¼Œæˆ‘ä»¬è®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„æ ‡ç­¾ï¼Œä½¿å¾—æ˜ å°„ä¸€è‡´ï¼›æ ‡ç­¾åˆ†å¸ƒä¸åŒï¼Œæˆ‘ä»¬ä½¿æ•°æ®åˆ†å¸ƒå°½é‡å‡è¡¡


* [æœºå™¨å­¦ä¹ æ•°æ®ä¸æ»¡è¶³åŒåˆ†å¸ƒï¼Œæ€ä¹ˆæ•´ï¼Ÿ](https://github.com/aialgorithm/Blog/issues/63)
    * ä»€ä¹ˆæ˜¯æ•°æ®ä¸æ»¡è¶³åŒåˆ†å¸ƒ
    * ä¸ºä»€ä¹ˆæ•°æ®ä¸æ»¡è¶³åŒåˆ†å¸ƒ
    * å¦‚ä½•æ£€æµ‹æ•°æ®æ»¡è¶³åŒåˆ†å¸ƒ
    * å¦‚ä½•è§£å†³æ•°æ®ä¸æ»¡è¶³åŒåˆ†å¸ƒ

* [Adversarial Validationè§£å†³åˆ†å¸ƒä¸ä¸€è‡´é—®é¢˜](https://www.kaggle.com/code/kevinbonnes/adversarial-validation/notebook)
    *  æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œç›®çš„æ˜¯åˆ†è¾¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ¥æºï¼Œè¿™é‡Œå‡è®¾ä½¿ç”¨AUCä½œä¸ºåˆ†ç±»ç²¾åº¦è¯„ä»·å‡½æ•°ã€‚
    *  å¦‚æœåˆ†ç±»æ¨¡å‹æ— æ³•åˆ†è¾¨æ ·æœ¬ï¼ˆAUCæ¥è¿‘0.5ï¼‰ï¼Œåˆ™è¯´æ˜è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®åˆ†å¸ƒæ¯”è¾ƒä¸€è‡´ï¼›
    *  å¦‚æœåˆ†ç±»æ¨¡å‹å¯ä»¥å¾ˆå¥½åˆ†è¾¨æ ·æœ¬ï¼ˆAUCæ¥è¿‘1ï¼‰ï¼Œåˆ™è¯´æ˜è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®åˆ†å¸ƒä¸å¤ªä¸€è‡´ï¼›
    *  å¯ä»¥é€šè¿‡å­—é¢åŒ¹é…æˆ–æ˜¯è¯­ä¹‰æ£€ç´¢ç±»æ–¹æ³•ä»æœªæ ‡æ³¨çš„æ•°æ®ä¸­ç­›é€‰å’Œæµ‹è¯•é›†ç›¸ä¼¼çš„æ•°æ®(åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å®è·µ)[åœºæ™¯å®è·µ(ä¸€)](https://kg-nlp.github.io/Algorithm-Project-Manual/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/%E5%9C%BA%E6%99%AF%E5%AE%9E%E8%B7%B5(%E4%B8%80).html)
    *  å…¶ä»–å‚è€ƒèµ„æ–™[paper](https://paperswithcode.com/paper/adversarial-validation-approach-to-concept)  [KaggleçŸ¥è¯†ç‚¹](https://zhuanlan.zhihu.com/p/136057721)


## ä¸“ä¸šæ¦‚å¿µ

* åå˜é‡    
    *  åå˜é‡æ˜¯ä¸åœ¨å—æ§åˆ¶ï¼ˆç ”ç©¶èŒƒå›´ï¼‰çš„è‡ªå˜é‡å†…ï¼Œä½†ä»ä¼šå½±å“ç ”ç©¶ç»“æœçš„å˜é‡ã€‚
    *  ç ”ç©¶æŸç§è‡ªå˜é‡å¯¹å› å˜é‡çš„å½±å“ï¼Œåˆ™å®éªŒè¿‡ç¨‹ä¸­é™¤ç ”ç©¶çš„è‡ªå˜é‡å› å˜é‡ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–å¾ˆå¤šå˜é‡å¯¹å®éªŒé€ æˆå½±å“ï¼Œè€Œè¿™äº›å…¶ä»–å˜é‡ä¸­ï¼Œå¯ä»¥è¢«æ§åˆ¶çš„å«æ§åˆ¶å˜é‡ï¼Œä¸å¯è¢«æ§åˆ¶çš„å«åå˜é‡
* è”åˆæ¦‚ç‡
    *  åŒ…å«å¤šä¸ªæ¡ä»¶ä¸”æ‰€æœ‰æ¡ä»¶åŒæ—¶æˆç«‹çš„æ¦‚ç‡ï¼Œè®°ä½œP(X=a,Y=b)æˆ–P(a,b)ï¼Œæœ‰çš„ä¹¦ä¸Šä¹Ÿä¹ æƒ¯è®°ä½œP(ab)
* è¾¹ç¼˜æ¦‚ç‡
    *  è¾¹ç¼˜æ¦‚ç‡æ˜¯ä¸è”åˆæ¦‚ç‡å¯¹åº”çš„ï¼ŒP(X=a)æˆ–P(Y=b)ï¼Œè¿™ç±»ä»…ä¸å•ä¸ªéšæœºå˜é‡æœ‰å…³çš„æ¦‚ç‡ç§°ä¸ºè¾¹ç¼˜æ¦‚ç‡
* æ¡ä»¶æ¦‚ç‡
    *  æ¡ä»¶æ¦‚ç‡è¡¨ç¤ºåœ¨æ¡ä»¶Y=bæˆç«‹çš„æƒ…å†µä¸‹ï¼ŒX=açš„æ¦‚ç‡ï¼Œè®°ä½œP(X=a\|Y=b)æˆ–P(a\|b)
* è´å¶æ–¯å…¬å¼
    *  å…ˆéªŒæ¦‚ç‡ï¼šçŸ¥é“åŸå› æ¨ç»“æœçš„ï¼ŒP(åŸå› )ã€P(ç»“æœ\|åŸå› )ç­‰
    *  åéªŒæ¦‚ç‡ï¼šæ ¹æ®ç»“æœæ¨åŸå› çš„ï¼ŒP(åŸå› \|ç»“æœ)ç­‰
    *  è´å¶æ–¯å…¬å¼è§£å†³çš„æ˜¯ä¸€äº›åŸå› Xæ— æ³•ç›´æ¥è§‚æµ‹ã€æµ‹é‡ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡å…¶ç»“æœYæ¥åæ¨å‡ºåŸå› Xçš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯çŸ¥é“ä¸€éƒ¨åˆ†å…ˆéªŒæ¦‚ç‡ï¼Œæ¥æ±‚åéªŒæ¦‚ç‡çš„é—®é¢˜ã€‚
* element-wise
    *  å¿«é€Ÿé€å…ƒç´ element-wise product = element-wise multiplication = Hadamard product ä¸¤ä¸ªçŸ©é˜µå¯¹åº”ä½ç½®å…ƒç´ è¿›è¡Œä¹˜ç§¯
* batch normalization
    * BN ç‹¬ç«‹åœ°è§„èŒƒåŒ–æ¯ä¸€ä¸ªè¾“å…¥ç»´åº¦`$x_i$` ï¼Œä½†è§„èŒƒåŒ–çš„å‚æ•°æ˜¯ä¸€ä¸ª mini-batch çš„ä¸€é˜¶ç»Ÿè®¡é‡å’ŒäºŒé˜¶ç»Ÿè®¡é‡ã€‚è¿™å°±è¦æ±‚ æ¯ä¸€ä¸ª mini-batch çš„ç»Ÿè®¡é‡æ˜¯æ•´ä½“ç»Ÿè®¡é‡çš„è¿‘ä¼¼ä¼°è®¡ï¼Œæˆ–è€…è¯´æ¯ä¸€ä¸ªmini-batchå½¼æ­¤ä¹‹é—´ï¼Œä»¥åŠå’Œæ•´ä½“æ•°æ®ï¼Œéƒ½åº”è¯¥æ˜¯è¿‘ä¼¼åŒåˆ†å¸ƒçš„ã€‚åˆ†å¸ƒå·®è·è¾ƒå°çš„mini-batchå¯ä»¥çœ‹åšæ˜¯ä¸ºè§„èŒƒåŒ–æ“ä½œå’Œæ¨¡å‹è®­ç»ƒå¼•å…¥äº†å™ªå£°ï¼Œå¯ä»¥å¢åŠ æ¨¡å‹çš„é²æ£’æ€§ï¼›ä½†å¦‚æœæ¯ä¸ªmini-batchçš„åŸå§‹åˆ†å¸ƒå·®åˆ«å¾ˆå¤§ï¼Œé‚£ä¹ˆä¸åŒ mini-batch çš„æ•°æ®å°†ä¼šè¿›è¡Œä¸ä¸€æ ·çš„æ•°æ®å˜æ¢ï¼Œè¿™å°±å¢åŠ äº†æ¨¡å‹è®­ç»ƒçš„éš¾åº¦ã€‚ 
    å› æ­¤ï¼ŒBN æ¯”è¾ƒé€‚ç”¨çš„åœºæ™¯æ˜¯ï¼šæ¯ä¸ª mini-batch æ¯”è¾ƒå¤§ï¼Œæ•°æ®åˆ†å¸ƒæ¯”è¾ƒæ¥è¿‘ã€‚åœ¨è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œè¦åšå¥½å……åˆ†çš„ shuffle. å¦åˆ™æ•ˆæœä¼šå·®å¾ˆå¤šã€‚
* Layer normalization
    *  LN é’ˆå¯¹å•ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œï¼Œä¸ä¾èµ–äºå…¶ä»–æ•°æ®,ä»¥ç”¨äº å°mini-batchåœºæ™¯



## ç›´è§‚ç†è§£

* **==åˆ†ç±»ä»»åŠ¡ä¸­æ ‡æ³¨æ•°æ®è¦†ç›–ä¸è¶³ä»¥åŠæ•°æ®åç½®é—®é¢˜==** [å‚è€ƒé“¾æ¥](https://zhuanlan.zhihu.com/p/636063567)

å¯¹äºæ•°æ®è¦†ç›–ä¸è¶³çš„é—®é¢˜,å¯ä»¥é€šè¿‡æŸ¥çœ‹æ ‡æ³¨æ•°æ®ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†å…³é”®è¯çš„å æ¯”æƒ…å†µæ¥åˆ¤æ–­æ ‡æ³¨æ•°æ®çš„åˆç†æ€§ï¼Œå¦‚æœå¯¹äºæŸä¸ªç±»åˆ«è®­ç»ƒé›†ä¸‹çš„å…³é”®è¯å æµ‹è¯•é›†è¶…è¿‡80%,æˆ‘ä»¬å°±è®¤ä¸ºæ ‡æ³¨æ•°æ®è¦†ç›–ç¨‹åº¦è¶³å¤Ÿ,å¯ä»¥ä½¿ç”¨å½“å‰æµ‹è¯•é›†éªŒè¯,å¦åˆ™éœ€è¦è¡¥å……è®­ç»ƒé›†

ä»¥ä¸‹æ˜¯åˆ†ææ•°æ®è¦†ç›–æƒ…å†µçš„ä»£ç 
```python
import os
import pandas as pd
from collections import defaultdict,Counter
import json
from tqdm import tqdm
import random
from copy import deepcopy
# from LAC import LAC
from paddlenlp import Taskflow
import matplotlib.pyplot as plt

def print_log(text:str):
    interval = int((100-len(text)*2)/2)
    print('-' * interval,text,'-' * interval)

def keyword_label_distribution():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_dir",
                        default=text_distribution_dir,  # è‡ªå®šä¹‰
                        type=str,
                        help="æ–‡æœ¬æ ‡ç­¾ç»Ÿè®¡è¾“å‡ºè·¯å¾„")
    parser.add_argument("--train_path", type=str, default=train_path, help="è®­ç»ƒé›†")
    parser.add_argument("--dev_path", type=str, default=dev_path, help="æµ‹è¯•é›†")
    parser.add_argument("--label_path", type=str, default=label_path, help="è®­ç»ƒé›†æ ‡ç­¾")
    parser.add_argument('--cnt_threshold', type=int, default=3, help='è‡³å°‘nå¥åŒ…å«æŸä¸ªå…³é”®è¯')
    parser.add_argument('--p_threshold', type=float, default=0.5,
                        help='è¾“å‡ºæœ€ä½åç½®åº¦é˜ˆå€¼')
    parser.add_argument('--cover_degree', type=bool, default=True,
                        help='è®­ç»ƒé›†å…³é”®è¯è¦†ç›–åº¦')
    parser.add_argument('--which_data', type=str, default='train',choices=['train','dev','all'],  # å…ˆè®¾ç½®ä¸ºtrain,å†è®¾ç½®ä¸ºdev
                        help='ç»Ÿè®¡æ•°æ®é›†ç±»åˆ«')
    args = parser.parse_args()
    class BiasWord(object):
        """
        Statistic the biased words in the dataset
        """
        def __init__(self, segments:list, labels:list, num_classes=24, cnt_threshold=3, p_threshold=0.85):
            self.cnt_threshold = cnt_threshold
            self.p_threshold = p_threshold
            self.num_classes = num_classes
            self.segments = segments  # [[]]
            self.labels = labels # []

        def process(self):
            self._get_dict()
            self._search_bias_word()
            print("number of bias_words:", len(self.word2label))
            return self.word2label, self.bias_word_cnt, self.label2word
        def _get_dict(self):
            self.word2sentids = defaultdict(set)  # å•è¯:(å¥å­ç´¢å¼•)
            self.label2word = defaultdict(dict)  # æ ‡ç­¾:{å•è¯:é¢‘æ•°(ä¸€ä¸ªå¥å­åªå–ä¸€æ¬¡)}
            self.sentid2words = defaultdict(set)  # å¥å­ç´¢å¼•:(å•è¯)
            label2word = defaultdict(list)
            for n, segs in enumerate(self.segments):
                for seg in segs:
                    self.word2sentids[seg].add(n)  # æ¯ä¸ªè¯å¯¹åº”çš„å¥å­ç´¢å¼•,å¥å­çš„ç´¢å¼•æ˜¯set(ä¸è€ƒè™‘ä¸€å¥å‡ºç°å¤šä¸ªç›¸åŒçš„è¯)
                self.sentid2words[n] = set(segs)  # æ¯ä¸ªå¥å­ç´¢å¼•å¯¹åº”çš„å¥å­åˆ†è¯,å¥å­åˆ†è¯åˆ—è¡¨set
                label2word[id2label[self.labels[n]]].extend(list(set(segs)))
            # ç»Ÿè®¡æ ‡ç­¾å¯¹åº”çš„å…³é”®è¯
            for k,v in label2word.items():
                self.label2word[k] = dict(sorted(dict(Counter(v)).items(),key=lambda x:x[1],reverse=True))
        def _search_bias_word(self):
            self.word2label = {}
            self.bias_word_cnt = {}
            for word, sentids in self.word2sentids.items():
                if len(sentids) >= self.cnt_threshold:  # å•è¯å‡ºç°çš„å¥å­æ•°é‡
                    cnts = [0] * self.num_classes
                    for sentid in sentids:
                        label_id = self.labels[sentid]  # å¥å­ç´¢å¼•å¯¹åº”çš„æ ‡ç­¾id
                        cnts[label_id] += 1
                    assert sum(cnts) != 0
                    max_cnt = max(cnts)  # ç»Ÿè®¡å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ ‡ç­¾
                    p = max_cnt / sum(cnts)  # ç»Ÿè®¡å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ ‡ç­¾å æ¯”
                    if p >= self.p_threshold:
                        #     self.word2label[word] = p  # åç½®è¯å­˜åœ¨çš„æœ€å¤§åç½®åº¦(æ²¡æœ‰æŒ‡å®šå“ªä¸ªæ ‡ç­¾)
                        #     self.bias_word_cnt[word] = len(sentids)  # åŒ…å«åç½®è¯çš„å¥å­æ•°é‡
                        self.bias_word_cnt[word] = len(sentids)  # åŒ…å«åç½®è¯çš„å¥å­æ•°é‡
                        # æ­¤å¤„ä¿®æ”¹å¢åŠ æ¯ä¸ªå…³é”®è¯å¯¹åº”çš„æ ‡ç­¾å’Œæ ‡ç­¾åˆ†å¸ƒ
                        # ç»Ÿè®¡å…³é”®è¯å¯¹åº”çš„æ ‡ç­¾ä»¥åŠå æ¯”
                        total_cnts = sum(cnts)
                        label_p_statistic = {}
                        for label_id,num in enumerate(cnts):
                            if num == 0: continue
                            label_p_statistic[id2label[label_id]] = round(num/total_cnts,2)
                        self.word2label[word] = dict(sorted(label_p_statistic.items(),key=lambda x:x[1],reverse=True))

    print_log('è¦†ç›–åº¦è®¡ç®—')  # ä¸‹é¢æ˜¯å¯¹ç”Ÿæˆçš„åˆ†è¯åç½®æ–‡ä»¶è¿›è¡Œå¤„ç†,ç”Ÿæˆè¦†ç›–åˆ†å¸ƒçš„æ–‡ä»¶
    if os.path.exists(os.path.join(args.output_dir, 'dev_label2word.json')) and \
                      os.path.exists(os.path.join(args.output_dir, 'train_label2word.json')) and args.cover_degree:
        print_log('è®¡ç®—è¦†ç›–åº¦')
        fr_train_dict = json.load(open(os.path.join(args.output_dir, 'train_label2word.json'),'r'))
        fr_dev_dict = json.load(open(os.path.join(args.output_dir, 'dev_label2word.json'),'r'))
        check_dict = defaultdict(dict)
        for k,v in tqdm(fr_dev_dict.items(),desc='è¦†ç›–åˆ†å¸ƒæå–'):
            temp_set = set(list(fr_train_dict[k].keys()))
            num = 0
            sum_num = len(v)  # æµ‹è¯•é›†è¯¥æ ‡ç­¾ä¸‹æ‰€æœ‰å…³é”®è¯
            for i in v.keys():
                if i in temp_set:  # æµ‹è¯•é›†ä¸‹æ‰€æœ‰æ•°æ®ä¸­ åœ¨è®­ç»ƒé›†ä¸­çš„å…³é”®è¯,å‰©ä¸‹çš„éƒ½ä¸åœ¨è¯¥èŒƒå›´å†…
                    num += 1
            check_dict[k] =  {'æµ‹è¯•æ ‡ç­¾å¯¹åº”å…³é”®è¯æ•°é‡':sum_num,
                           'è®­ç»ƒé›†æœªè¦†ç›–å…³é”®è¯æ•°é‡':sum_num-num,
                           'æœªè¦†ç›–æ¯”ä¾‹':(sum_num-num)/sum_num,
                            'è¦†ç›–æ¯”ä¾‹':num/sum_num}
        fw_proportion = open(os.path.join(args.output_dir, 'word_label_proportion.json'),'w')
        json.dump(check_dict, fw_proportion, indent=2, ensure_ascii=False)
        fw_proportion.close()
        df = pd.DataFrame.from_dict(check_dict,orient='index')
        plt.figure(dpi=300, figsize=(10, 8))
        df.plot.barh(y=['æœªè¦†ç›–æ¯”ä¾‹','è¦†ç›–æ¯”ä¾‹'],stacked=True)
        plt.show()
        # print(df.head())

        return  # æ‰§è¡Œå®Œç›´æ¥è¿”å›

    print_log('æå–å…³é”®è¯')
    with open(stopwords_path, 'r', encoding='utf-8') as f:  #
        stopword_list = set([word.strip('\n') for word in f.readlines()])
    ner_fast = Taskflow("ner", mode='fast', batch_size=1, user_dict=userdict_path)
    pos_flag = set(['n', 'nz', 'vn', 'an', 'v'])
    # è·å–label2idå’Œid2label,ä»¥åŠæ•°é‡num_classes
    label2id = {}
    id2label = {}
    for id, label in enumerate(open(args.label_path, 'r').readlines()):
        label2id[label.strip()] = id
        id2label[id] = label.strip()
    num_classes = len(label2id)
    segments = []
    labels = []
    lines = []

    if args.train_path and (args.which_data == 'train' or args.which_data == 'all'):
        with open(args.train_path, 'r') as f:  # è¿™éƒ¨åˆ†æ ¹æ®è‡ªå·±çš„æ–‡ä»¶ä¿®æ”¹å¤„ç†æ–¹æ³•,ç›®æ ‡æ˜¯ç”Ÿæˆåˆ†è¯
            for line in tqdm(list(f)[1:],desc='è®­ç»ƒé›†åˆ†è¯'):
                lines.append(line)
                task_type,task_name,prefix_name,label = line.strip().split('\t')  # æˆ‘è¿™é‡Œçš„æ•°æ®æ˜¯å››åˆ—
                if not task_name:task_name='ç©º'
                if not prefix_name:prefix_name='ç©º'
                seg_res = ner_fast([task_name, prefix_name])
                task_prefix_segs = []
                for i in seg_res[0]+seg_res[1]:
                    if i[0] not in stopword_list and len(i[0]) > 1 and i[1] in pos_flag:
                        task_prefix_segs.append(i[0])
                segments.append(task_prefix_segs)
                labels.append(label2id[label])
    if args.dev_path and (args.which_data == 'dev' or args.which_data == 'all'):
        with open(args.dev_path, 'r') as f:
            for line in tqdm(list(f)[1:],desc='éªŒè¯é›†åˆ†è¯'):
                lines.append(line)
                task_type, task_name, prefix_name, label = line.strip().split('\t')
                if not task_name:task_name='ç©º'
                if not prefix_name:prefix_name='ç©º'
                seg_res = ner_fast([task_name, prefix_name])
                task_prefix_segs = []
                for i in seg_res[0] + seg_res[1]:
                    if i[0] not in stopword_list and len(i[0]) > 1 and i[1] in pos_flag:
                        task_prefix_segs.append(i[0])
                segments.append(task_prefix_segs)
                labels.append(label2id[label])

    # get bias_words
    print_log('è·å–åç½®æ•°æ®')
    biasword = BiasWord(segments, labels, num_classes=num_classes, cnt_threshold=args.cnt_threshold,
                        p_threshold=args.p_threshold)
    # b_words: biased words, dict
    # b_word_cnt: count of biased words, dict
    # sentids2words: sentence index to words, dict
    # b_words, b_word_cnt, sentids2words = biasword.process()
    print_log('å†™å…¥æ–‡ä»¶')
    word2label, b_word_cnt, label2word = biasword.process()
    bias_word = 'word2label.json'
    bias_word_cnt = 'bias_word_cnt.json'
    bias_babel = 'label2word.json'
    if args.which_data=='all':
        pass
    elif args.which_data=='train':
        bias_word = 'train_word2label.json'
        bias_word_cnt = 'train_bias_word_cnt.json'
        bias_babel = 'train_label2word.json'
    elif args.which_data=='dev':
        bias_word = 'dev_word2label.json'
        bias_word_cnt = 'dev_bias_word_cnt.json'
        bias_babel = 'dev_label2word.json'

    # save result to output_dir
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    with open(os.path.join(args.output_dir, bias_word), 'w') as f:
        json.dump(word2label, f, indent=2, ensure_ascii=False)
    with open(os.path.join(args.output_dir, bias_word_cnt), 'w') as f:
        json.dump(b_word_cnt, f, indent=2, ensure_ascii=False)
    with open(os.path.join(args.output_dir, bias_babel), 'w') as f:
        json.dump(label2word, f, indent=2, ensure_ascii=False)
    # with open(os.path.join(args.output_dir, "sentids2words2words.json"), 'w') as f:
    #     for k, v in sentids2words2words.items():
    #         sentids2words2words[k] = list(v)
    #     json.dump(sentids2words, f, indent=2, ensure_ascii=False)
```

* æ–‡ä»¶ç¤ºä¾‹

train_word2label.json

```
{
  "å•è¯1": {
    "æ ‡ç­¾1": 0.62,
    "æ ‡ç­¾2": 0.31,
    "æ ‡ç­¾3": 0.08
  },
  "å•è¯2":{...}
```

train_bias_word_cnt.json


```
{
  "å•è¯1": 13,
  "å•è¯2": 1340,
  ...
 }
```

train_label2word.json

```
{
  "æ ‡ç­¾1": {
    "å•è¯1": 726,
    "å•è¯2": 520,
    ...
    },
  "æ ‡ç­¾2":{...}
```

word_label_proportion

```
{
  "æ ‡ç­¾1": {
    "æµ‹è¯•æ ‡ç­¾å¯¹åº”å…³é”®è¯æ•°é‡": 215,
    "è®­ç»ƒé›†æœªè¦†ç›–å…³é”®è¯æ•°é‡": 42,
    "æœªè¦†ç›–æ¯”ä¾‹": 0.19534883720930232,
    "è¦†ç›–æ¯”ä¾‹": 0.8046511627906977
  },
  "æ ‡ç­¾2": {
    "æµ‹è¯•æ ‡ç­¾å¯¹åº”å…³é”®è¯æ•°é‡": 179,
    "è®­ç»ƒé›†æœªè¦†ç›–å…³é”®è¯æ•°é‡": 21,
    "æœªè¦†ç›–æ¯”ä¾‹": 0.11731843575418995,
    "è¦†ç›–æ¯”ä¾‹": 0.88268156424581
  }
  ...
 }
 è¿™ä¸ªç»“æœè¯´æ˜è®­ç»ƒé›†è¦†ç›–åº¦è¿˜å¯ä»¥
```


## å‚è€ƒé“¾æ¥

* [åœ¨è¿ç§»å­¦ä¹ ä¸­ï¼Œè¾¹ç¼˜æ¦‚ç‡åˆ†å¸ƒå’Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæœ‰ä»€ä¹ˆå«ä¹‰ï¼Ÿ](https://www.zhihu.com/question/293820673)
* [è¯¦è§£æ·±åº¦å­¦ä¹ ä¸­çš„Normalizationï¼ŒBN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
* [æ·±åº¦å­¦ä¹ ä¸­ Batch Normalizationä¸ºä»€ä¹ˆæ•ˆæœå¥½ï¼Ÿ](https://www.zhihu.com/question/38102762/answer/85238569)
* [å…¨è¿æ¥å±‚çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ](https://www.zhihu.com/question/41037974/answer/150522307)
* [numpyçš„æ•°ç»„å’Œå‘é‡åŒ–è®¡ç®—](https://pydata.readthedocs.io/zh/latest/chapters/p04_numpy_basics_arrays_and_vectorized_computation.html)
* [BatchNormå’ŒLayerNormâ€”â€”é€šä¿—æ˜“æ‡‚çš„ç†è§£](https://blog.csdn.net/Little_White_9/article/details/123345062)

* [BatchNormä¸LayerNormçš„ç†è§£](https://zhuanlan.zhihu.com/p/608655896)
* [ä¸€æ–‡ææ‡‚Batch Normalization,Layer/Instance/GroupNorm](https://zhuanlan.zhihu.com/p/152232203)
* [AIç–¯ç‹‚è¿›é˜¶â€”â€”æ­£åˆ™åŒ–ç¯‡æ’­](https://baijiahao.baidu.com/s?id=1653085297096293714&wfr=spider&for=pc)
* [å…³äºweight_decayçš„æ·±åº¦åˆ†æ](https://zhuanlan.zhihu.com/p/339448370)
* [æ·±åº¦å­¦ä¹ å¿…é¡»æŒæ¡çš„ 13 ç§æ¦‚ç‡åˆ†å¸ƒ](https://mp.weixin.qq.com/s/gA581-ksmTtfJsXWG1ogPQ)